import json
import time
import requests
import logging
import base64
import random
import os
from typing import Dict, Any, Optional

# é»˜è®¤é…ç½®ï¼ˆä¾›å…¶ä»–æ¨¡å—ç›´æ¥å¼•ç”¨ï¼Œä¸å†å¼ºåˆ¶ä¾èµ–ç¯å¢ƒå˜é‡ï¼‰
DEFAULT_GOLIATH_ADDR = "https://rockmgrgo.wenxiaobai.com/rockopenai/v1/web_search"
DEFAULT_GOLIATH_TOKEN = "rock-ShWAczRU3K8Pk12p6NQalR3JP1GD7pUhemTwNnmD7PW26"  # ç§æœ‰ç¯å¢ƒä½¿ç”¨
DEFAULT_GOLIATH_TIMEOUT_MS = 120000
DEFAULT_GOLIATH_MAX_RETRY = 2
DEFAULT_GOLIATH_BIZ_DEF = "ref_fetch"


# è®¾ç½®æ—¥å¿—
logger = logging.getLogger('GoliathTool')
logger.setLevel(logging.INFO)

formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)


class GoliathTool:
    """Goliathç½‘é¡µçˆ¬å–å·¥å…·"""
    
    def __init__(
        self,
        addr: str,
        rock_token: str,
        timeout_ms: int = 120000,
        max_retry: int = 2,
        biz_def: str = "test",
    ):
        self.addr = addr
        self.rock_token = rock_token
        self.timeout_ms = timeout_ms
        self.max_retry = max_retry
        self.biz_def = biz_def
        self.model_name = "yuanshi/goliath/retrieve"

    def retrieve(
        self,
        url: str,
        retrieve_type: str = "MARKDOWN_RICH",
    ) -> Dict[str, Any]:
        """
        çˆ¬å–å’Œè§£æç½‘é¡µå†…å®¹
        
        Args:
            url: è¦çˆ¬å–çš„ç½‘å€
            retrieve_type: çˆ¬å–ç±»å‹ (RAW, TEXT, MARKDOWN, IMAGE, PDF, VIDEO_SUMMARY, MARKDOWN_RICH)
            
        Returns:
            åŒ…å«çˆ¬å–ç»“æœçš„å­—å…¸
        """
        for attempt in range(self.max_retry):
            request_id = f"goliath_retrieve_{int(time.time()*1000)}"
            
            # ä½¿ç”¨ç®€åŒ–çš„payloadç»“æ„ï¼Œä¸goliath_demoä¿æŒä¸€è‡´
            payload = {
                "biz_def": self.biz_def,
                "url": url,
                "timeout_ms": self.timeout_ms,
                "retrieve_type": retrieve_type,
                "model": self.model_name
            }
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.rock_token}",
                "rock-request-id": request_id,
            }
            
            try:
                logger.info(f"æ­£åœ¨çˆ¬å–: {url}")
                response = requests.post(self.addr, data=json.dumps(payload), headers=headers, timeout=(self.timeout_ms/1000))
                return self._handle_response(response)
                    
            except Exception as e:
                logger.error(f"çˆ¬å–å°è¯•ç¬¬ {attempt + 1} æ¬¡æ—¶å‡ºé”™: {e}")
                if attempt < self.max_retry - 1:
                    logger.info("ç­‰å¾…5såé‡è¯•...")
                    time.sleep(5)
                continue
                
        return {"error": "æ‰€æœ‰é‡è¯•å°è¯•å‡å¤±è´¥"}

    def _handle_response(self, response: requests.Response) -> Dict[str, Any]:
        """å¤„ç†å“åº”"""
        try:
            response_data = response.json()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰resultå­—æ®µ
            if "result" in response_data:
                result = response_data["result"]
                
                # è§£ç base64å†…å®¹
                content = result.get("content", "")
                if content:
                    try:
                        decoded_content = base64.b64decode(content).decode('utf-8')
                        result["content"] = decoded_content
                    except Exception as e:
                        logger.warning(f"è§£ç contentå¤±è´¥: {e}")

                return {
                    "success": True,
                    "request_id": response_data.get("request_id"),
                    "debug_string": response_data.get("debug_string", ""),
                    "result": result
                }
            else:
                return {
                    "success": False,
                    "error": "å“åº”ä¸­æœªæ‰¾åˆ°resultå­—æ®µ",
                    "raw_response": response_data
                }

        except Exception as e:
            logger.error(f"è§£æå“åº”æ—¶å‡ºé”™: {e}")
            return {
                "success": False,
                "error": f"è§£æå“åº”å¤±è´¥: {e}",
                "raw_text": response.text
            }

    def __call__(self, url: str, retrieve_type="MARKDOWN_RICH", **kwargs) -> Dict[str, Any]:
        """
        è°ƒç”¨æ¥å£
        
        Args:
            url: è¦çˆ¬å–çš„ç½‘å€
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            çˆ¬å–ç»“æœ
        """
        try:
            response_dict = self.retrieve(url, retrieve_type, **kwargs)
            if response_dict.get("success"):
                return {
                    'success': True,
                    'url': response_dict.get("result", {}).get("url", ""),
                    'title': response_dict.get("result", {}).get("title", ""),
                    'description': response_dict.get("result", {}).get("decscription", ""),
                    'content': response_dict.get("result", {}).get("content", ""),
                    'publish_time': response_dict.get("result", {}).get("publish_time", "")
                }
            else:
                return {
                    'success': False,
                    'url': url,
                    'title': '',
                    'content': '',
                    "error": response_dict.get("error", "Unknown error")
                }
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±è´¥: {e}")
            return {
                'success': False,
                'url': url,
                'title': '',
                'content': '',
                'error': str(e)
            }


def build_default_goliath_tool() -> GoliathTool:
    """æä¾›ä¸€ä¸ªå¯å¤ç”¨çš„é»˜è®¤å®ä¾‹ï¼Œä¾›å¼•ç”¨æŠ“å–è„šæœ¬ç›´æ¥è°ƒç”¨ã€‚"""
    return GoliathTool(
        addr=DEFAULT_GOLIATH_ADDR,
        rock_token=DEFAULT_GOLIATH_TOKEN,
        timeout_ms=DEFAULT_GOLIATH_TIMEOUT_MS,
        max_retry=DEFAULT_GOLIATH_MAX_RETRY,
        biz_def=DEFAULT_GOLIATH_BIZ_DEF,
    )


def test_batch_crawl():
    """æµ‹è¯•æ‰¹é‡çˆ¬å–åŠŸèƒ½ï¼šä»web_pages.jsonlé‡‡æ ·1000æ¡æ•°æ®å¹¶çˆ¬å–"""
    print("ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å–æµ‹è¯•...")
    
    # æ–‡ä»¶è·¯å¾„
    source_file = "/mnt/jfs/xubenfeng/ys_agent_dev/thinkingagent/data/context_reward/data/web_pages.jsonl"
    sample_file = "/mnt/jfs/xubenfeng/ys_agent_dev/thinkingagent/data/context_reward/data/web_pages_sample1000.jsonl"
    output_dir = "/mnt/jfs/xubenfeng/ys_agent_dev/ys_agent_dev/tools/data"
    
    # ç¬¬ä¸€æ­¥ï¼šä»åŸå§‹æ–‡ä»¶é‡‡æ ·1000æ¡æ•°æ®
    # print("ğŸ“ æ­¥éª¤1: ä»åŸå§‹æ–‡ä»¶é‡‡æ ·1000æ¡æ•°æ®...")
    
    # è¯»å–æ‰€æœ‰æ•°æ®
    # all_data = []
    # with open(source_file, 'r', encoding='utf-8') as f:
    #     for line in f:
    #         line = line.strip()
    #         if line:
    #             try:
    #                 data = json.loads(line)
    #                 all_data.append(data)
    #             except json.JSONDecodeError:
    #                 continue
    
    # print(f"åŸå§‹æ–‡ä»¶å…±æœ‰ {len(all_data)} æ¡æ•°æ®")
    
    # # éšæœºé‡‡æ ·100æ¡
    # sample_size = min(1000, len(all_data))
    # sampled_data = random.sample(all_data, sample_size)
    
    # # ä¿å­˜é‡‡æ ·æ•°æ®
    # with open(sample_file, 'w', encoding='utf-8') as f:
    #     for data in sampled_data:
    #         f.write(json.dumps(data, ensure_ascii=False) + '\n')
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # ç¬¬ä¸€æ­¥ï¼šè¯»å–é‡‡æ ·1000æ¡æ•°æ®
    
    # è¯»å–æ‰€æœ‰æ•°æ®
    sampled_data = []
    with open(sample_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    data = json.loads(line)
                    sampled_data.append(data)
                except json.JSONDecodeError:
                    continue
    
    sample_size = len(sampled_data)
    print(f"âœ… å·²é‡‡æ · {sample_size} æ¡æ•°æ®å¹¶ä¿å­˜åˆ°: {sample_file}")
    
    # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨GoliathToolçˆ¬å–è¿™äº›ç½‘é¡µ
    print("\nğŸ•·ï¸  æ­¥éª¤2: å¼€å§‹æ‰¹é‡çˆ¬å–ç½‘é¡µ...")
    
    # åˆ›å»ºå·¥å…·å®ä¾‹
    tool = GoliathTool(
        addr="https://rockmgrgo.wenxiaobai.com/rockopenai/v1/web_search",
        rock_token="rock-ShWAczRU3K8Pk12p6NQalR3JP1GD7pUhemTwNnmD7PW26",
        timeout_ms=60000, 
        max_retry=3, 
        biz_def="batch_test"
    )
    
    # çˆ¬å–ç»“æœ
    crawl_results = []
    success_count = 0
    fail_count = 0
    
    for i, data in enumerate(sampled_data):
        url = data.get("url", "")
        title = data.get("title", "")
        
        print(f"[{i+1}/{sample_size}] çˆ¬å–: {title[:50]}...")
        
        start_time = time.time()
        result = tool.retrieve(url=url, retrieve_type="MARKDOWN_RICH")
        end_time = time.time()
        
        crawl_time = end_time - start_time
        
        if result.get("success"):
            success_count += 1
            content = result.get("result", {}).get("content", "")
            crawl_results.append({
                "index": i + 1,
                "url": url,
                "original_title": title,
                "crawl_success": True,
                "crawl_time": round(crawl_time, 2),
                "content_length": len(content),
                "content": content,
                "request_id": result.get("request_id", ""),
                "debug_info": result.get("debug_string", "")
            })
            print(f"  âœ… æˆåŠŸï¼Œè€—æ—¶: {crawl_time:.2f}ç§’ï¼Œå†…å®¹: {len(content)}å­—ç¬¦")
        else:
            fail_count += 1
            crawl_results.append({
                "index": i + 1,
                "url": url,
                "original_title": title,
                "crawl_success": False,
                "crawl_time": round(crawl_time, 2),
                "content_length": 0,
                "content": "",
                "error": result.get("error", "Unknown error")
            })
            print(f"  âŒ å¤±è´¥ï¼Œè€—æ—¶: {crawl_time:.2f}ç§’ï¼Œé”™è¯¯: {result.get('error', 'Unknown')}")
        
        # æ·»åŠ çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
        if i < sample_size - 1:  # æœ€åä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
            time.sleep(1)
    
    # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜çˆ¬å–ç»“æœ
    print(f"\nğŸ’¾ æ­¥éª¤3: ä¿å­˜çˆ¬å–ç»“æœ...")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(output_dir, f"goliath_crawl_results_{int(time.time())}.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(crawl_results, f, ensure_ascii=False, indent=2)
    
    # ç»Ÿè®¡æŠ¥å‘Š
    total_time = sum(r["crawl_time"] for r in crawl_results)
    avg_time = total_time / len(crawl_results) if crawl_results else 0
    total_content = sum(r["content_length"] for r in crawl_results if r["crawl_success"])
    
    print(f"\nğŸ“Š æ‰¹é‡çˆ¬å–å®Œæˆï¼ç»Ÿè®¡æŠ¥å‘Š:")
    print(f"  æ€»è®¡: {sample_size} ä¸ªç½‘é¡µ")
    print(f"  æˆåŠŸ: {success_count} ä¸ª")
    print(f"  å¤±è´¥: {fail_count} ä¸ª")
    print(f"  æˆåŠŸç‡: {success_count/sample_size*100:.1f}%")
    print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"  å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’/ä¸ª")
    print(f"  æ€»å†…å®¹: {total_content} å­—ç¬¦")
    print(f"  ç»“æœæ–‡ä»¶: {results_file}")
    print("=" * 60)


if __name__ == "__main__":
    # åˆ›å»º Goliath å·¥å…·å®ä¾‹
    tool = build_default_goliath_tool()

    print("=== æµ‹è¯•ç½‘é¡µçˆ¬å–åŠŸèƒ½ ===")
    # æµ‹è¯•çˆ¬å–åŠŸèƒ½
    result = tool(
        # url="https://www.axtonliu.com/ai-ethics-trolley-problem-experiment/",
        # url="https://www.zhihu.com/question/442922646/answer/1729264160"
        # url="https://www.zhihu.com/question/399526996/answer/1266743626"
        url="https://36kr.com/p/3429634800012934"
        )
    print(result)
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    if result.get("success"):
        output_dir = "/home/inter_wangpengyu/Wiki_Challenge/wiki_data/"
        os.makedirs(output_dir, exist_ok=True)
        
        title = result.get("title", "untitled").replace("/", "_").replace("\\", "_")
        content = result.get("content", "")
        
        filename = f"{title}.md"
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ… å†…å®¹å·²ä¿å­˜åˆ°: {filepath}")
    
    # if result.get("success"):
    #     print("çˆ¬å–æˆåŠŸ!")
    #     print(f"Request ID: {result.get('request_id')}")
    #     print(f"Debug: {result.get('debug_string')}")
    #     content = result.get("result", {}).get("content", "")
    #     print(f"å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
    #     print("å†…å®¹é¢„è§ˆ:")
    #     print(content[:500] + "..." if len(content) > 500 else content)
    # else:
    #     print("çˆ¬å–å¤±è´¥:")
    #     print(json.dumps(result, ensure_ascii=False, indent=2))

    # æ‰¹é‡æµ‹è¯•
    # print("\n" + "="*60)
    # test_batch_crawl()



