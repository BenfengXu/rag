import re
import json
import jsonlines
import os
from local_models import oss_llm_complete

def batch_eval(query_file, result1_file, result2_file, output_file_path):
    """æ‰¹é‡è¯„ä¼°ä¸¤ä¸ªç»“æœæ–‡ä»¶"""
    
    # è¯»å–æŸ¥è¯¢
    with open(query_file, "r", encoding="utf-8") as f:
        data = f.read()
    queries = re.findall(r"- Question \d+: (.+)", data)

    # è¯»å–ç»“æœ1
    with open(result1_file, "r", encoding="utf-8") as f:
        answers1 = json.load(f)
    answers1 = [i["result"] for i in answers1]

    # è¯»å–ç»“æœ2
    with open(result2_file, "r", encoding="utf-8") as f:
        answers2 = json.load(f)
    answers2 = [i["result"] for i in answers2]

    print(f"æŸ¥è¯¢æ•°é‡: {len(queries)}")
    print(f"ç»“æœ1æ•°é‡: {len(answers1)}")
    print(f"ç»“æœ2æ•°é‡: {len(answers2)}")

    if len(queries) != len(answers1) or len(queries) != len(answers2):
        print("âŒ æŸ¥è¯¢å’Œç»“æœæ•°é‡ä¸åŒ¹é…ï¼")
        return

    evaluations = []
    
    for i, (query, answer1, answer2) in enumerate(zip(queries, answers1, answers2)):
        print(f"æ­£åœ¨è¯„ä¼°ç¬¬ {i+1}/{len(queries)} ä¸ªæŸ¥è¯¢...")
        
        sys_prompt = """
        ---Role---
        You are an expert tasked with evaluating two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.
        """

        prompt = f"""
        You will evaluate two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.

        - **Comprehensiveness**: How much detail does the answer provide to cover all aspects and details of the question?
        - **Diversity**: How varied and rich is the answer in providing different perspectives and insights on the question?
        - **Empowerment**: How well does the answer help the reader understand and make informed judgments about the topic?

        For each criterion, choose the better answer (either Answer 1 or Answer 2) and explain why. Then, select an overall winner based on these three categories.

        Here is the question:
        {query}

        Here are the two answers:

        **Answer 1:**
        {answer1}

        **Answer 2:**
        {answer2}

        Evaluate both answers using the three criteria listed above and provide detailed explanations for each criterion.

        Output your evaluation in the following JSON format:

        {{
            "Comprehensiveness": {{
                "Winner": "[Answer 1 or Answer 2]",
                "Explanation": "[Provide explanation here]"
            }},
            "Diversity": {{
                "Winner": "[Answer 1 or Answer 2]",
                "Explanation": "[Provide explanation here]"
            }},
            "Empowerment": {{
                "Winner": "[Answer 1 or Answer 2]",
                "Explanation": "[Provide explanation here]"
            }},
            "Overall Winner": {{
                "Winner": "[Answer 1 or Answer 2]",
                "Explanation": "[Summarize why this answer is the overall winner based on the three criteria]"
            }}
        }}
        """

        try:
            # ä½¿ç”¨OSS APIè¿›è¡Œè¯„ä¼°
            evaluation_result = oss_llm_complete(
                prompt=prompt,
                system_prompt=sys_prompt,
                max_tokens=2048,
                temperature=0.1  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„è¯„ä¼°
            )
            
            # å°è¯•è§£æJSONç»“æœ
            try:
                # æå–JSONéƒ¨åˆ†
                json_start = evaluation_result.find('{')
                json_end = evaluation_result.rfind('}') + 1
                if json_start != -1 and json_end > json_start:
                    json_str = evaluation_result[json_start:json_end]
                    evaluation_data = json.loads(json_str)
                else:
                    evaluation_data = {"error": "æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼", "raw_response": evaluation_result}
            except json.JSONDecodeError:
                evaluation_data = {"error": "JSONè§£æå¤±è´¥", "raw_response": evaluation_result}
            
            evaluation_entry = {
                "query_index": i,
                "query": query,
                "evaluation": evaluation_data
            }
            
            evaluations.append(evaluation_entry)
            print(f"âœ… ç¬¬ {i+1} ä¸ªæŸ¥è¯¢è¯„ä¼°å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ ç¬¬ {i+1} ä¸ªæŸ¥è¯¢è¯„ä¼°å¤±è´¥: {e}")
            evaluation_entry = {
                "query_index": i,
                "query": query,
                "evaluation": {"error": str(e)}
            }
            evaluations.append(evaluation_entry)

    # ä¿å­˜è¯„ä¼°ç»“æœ
    with open(output_file_path, "w", encoding="utf-8") as output_file:
        json.dump(evaluations, output_file, ensure_ascii=False, indent=2)
    
    print(f"âœ… è¯„ä¼°å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file_path}")
    
    # ç»Ÿè®¡ç»“æœ
    analyze_evaluation_results(evaluations)

def analyze_evaluation_results(evaluations):
    """åˆ†æè¯„ä¼°ç»“æœ"""
    print("\n" + "="*50)
    print("è¯„ä¼°ç»“æœç»Ÿè®¡")
    print("="*50)
    
    total_valid = 0
    criteria_stats = {
        "Comprehensiveness": {"Answer 1": 0, "Answer 2": 0},
        "Diversity": {"Answer 1": 0, "Answer 2": 0},
        "Empowerment": {"Answer 1": 0, "Answer 2": 0},
        "Overall Winner": {"Answer 1": 0, "Answer 2": 0}
    }
    
    for eval_entry in evaluations:
        evaluation = eval_entry.get("evaluation", {})
        if "error" not in evaluation:
            total_valid += 1
            for criterion in criteria_stats:
                winner = evaluation.get(criterion, {}).get("Winner", "")
                if winner in ["Answer 1", "Answer 2"]:
                    criteria_stats[criterion][winner] += 1
    
    print(f"æœ‰æ•ˆè¯„ä¼°æ•°é‡: {total_valid}/{len(evaluations)}")
    
    for criterion, stats in criteria_stats.items():
        total = stats["Answer 1"] + stats["Answer 2"]
        if total > 0:
            pct1 = (stats["Answer 1"] / total) * 100
            pct2 = (stats["Answer 2"] / total) * 100
            print(f"\n{criterion}:")
            print(f"  Answer 1: {stats['Answer 1']} ({pct1:.1f}%)")
            print(f"  Answer 2: {stats['Answer 2']} ({pct2:.1f}%)")

def main():
    """ä¸»å‡½æ•°"""
    # ä»ç¯å¢ƒå˜é‡è¯»å–ç±»åˆ«ï¼Œé»˜è®¤ä¸ºagriculture
    cls = os.getenv("CLASS", "agriculture")
    mode1 = "naive"      # ç¬¬ä¸€ä¸ªè¦æ¯”è¾ƒçš„æ¨¡å¼
    mode2 = "hybrid"     # ç¬¬äºŒä¸ªè¦æ¯”è¾ƒçš„æ¨¡å¼
    
    print(f"ğŸ“Š è¯„ä¼°æ•°æ®é›†ç±»åˆ«: {cls}")
    print(f"ğŸ“ˆ æ¯”è¾ƒæ¨¡å¼: {mode1} vs {mode2}")
    
    # æ–‡ä»¶è·¯å¾„
    query_file = f"../exp_results/data/questions/{cls}_questions.txt"
    result1_file = f"../exp_results/data/results/{cls}_{mode1}_results.json"
    result2_file = f"../exp_results/data/results/{cls}_{mode2}_results.json"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    files_to_check = [query_file, result1_file, result2_file]
    for file_path in files_to_check:
        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            print("è¯·ç¡®ä¿å·²è¿è¡Œå®Œæ•´çš„å®éªŒæµç¨‹")
            return
    
    # åˆ›å»ºè¯„ä¼°ç»“æœç›®å½•
    eval_dir = "../exp_results/data/evaluations"
    os.makedirs(eval_dir, exist_ok=True)
    
    # è¾“å‡ºæ–‡ä»¶
    output_file = f"{eval_dir}/{cls}_{mode1}_vs_{mode2}_evaluation.json"
    
    print(f"å¼€å§‹è¯„ä¼°: {mode1} vs {mode2}")
    print(f"ç±»åˆ«: {cls}")
    print(f"æŸ¥è¯¢æ–‡ä»¶: {query_file}")
    print(f"ç»“æœ1æ–‡ä»¶: {result1_file}")
    print(f"ç»“æœ2æ–‡ä»¶: {result2_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    batch_eval(query_file, result1_file, result2_file, output_file)

if __name__ == "__main__":
    main()
