import os
import json
import glob
import argparse
from collections import defaultdict


def load_jsonl_data(file_path):
    """åŠ è½½ JSONL æ–‡ä»¶æ•°æ®"""
    data = []
    if not os.path.exists(file_path):
        return data
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        data.append(json.loads(line))
                    except json.JSONDecodeError as e:
                        print(f"JSONè§£æé”™è¯¯ {file_path} ç¬¬{line_num}è¡Œ: {e}")
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶é”™è¯¯ {file_path}: {e}")
    
    return data


def create_enhanced_contexts(corpus_directory):
    """åŸºäºæ‰€æœ‰11ä¸ªè¡¨åˆ›å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡"""
    print(f"ğŸ“Š ä» {corpus_directory} åŠ è½½æ‰€æœ‰æ•°æ®è¡¨...")
    
    # 1. åŠ è½½æ‰€æœ‰æ•°æ®è¡¨
    data_tables = {}
    table_names = [
        'docs', 'sections', 'sentences', 'passages', 'wikilinks', 
        'references', 'ref_mentions', 'ext_docs', 'ext_passages', 
        'ref2ext', 'claims'
    ]
    
    for table_name in table_names:
        file_path = os.path.join(corpus_directory, f"{table_name}.jsonl")
        data_tables[table_name] = load_jsonl_data(file_path)
        print(f"  âœ… {table_name}: {len(data_tables[table_name])} æ¡è®°å½•")
    
    # 2. æ„å»ºç´¢å¼•ç”¨äºå¿«é€ŸæŸ¥æ‰¾
    print("ğŸ”— æ„å»ºæ•°æ®ç´¢å¼•...")
    
    # æ–‡æ¡£ç´¢å¼•
    docs_by_id = {doc['doc_id']: doc for doc in data_tables['docs']}
    
    # ç« èŠ‚ç´¢å¼•
    sections_by_doc = defaultdict(list)
    for section in data_tables['sections']:
        sections_by_doc[section['doc_id']].append(section)
    
    # å¥å­ç´¢å¼•
    sentences_by_doc = defaultdict(list)
    sentences_by_section = defaultdict(list)
    for sentence in data_tables['sentences']:
        sentences_by_doc[sentence['doc_id']].append(sentence)
        sentences_by_section[sentence['section_id']].append(sentence)
    
    # å¼•ç”¨ç´¢å¼•
    refs_by_doc = defaultdict(list)
    for ref in data_tables['references']:
        refs_by_doc[ref['doc_id']].append(ref)
    
    # å¼•ç”¨æåŠç´¢å¼•
    ref_mentions_by_doc = defaultdict(list)
    for mention in data_tables['ref_mentions']:
        ref_mentions_by_doc[mention['doc_id']].append(mention)
    
    # ç»´åŸºé“¾æ¥ç´¢å¼•
    wikilinks_by_doc = defaultdict(list)
    for link in data_tables['wikilinks']:
        wikilinks_by_doc[link['doc_id']].append(link)
    
    # å¤–éƒ¨æ–‡æ¡£ç´¢å¼•
    ext_docs_by_id = {ext_doc['ext_doc_id']: ext_doc for ext_doc in data_tables['ext_docs']}
    
    # å¤–éƒ¨æ®µè½ç´¢å¼•
    ext_passages_by_doc = defaultdict(list)
    for ext_passage in data_tables['ext_passages']:
        ext_passages_by_doc[ext_passage['ext_doc_id']].append(ext_passage)
    
    # ref2ext æ˜ å°„
    ext_docs_by_ref = {}
    for mapping in data_tables['ref2ext']:
        ext_docs_by_ref[mapping['ref_id']] = mapping['ext_doc_id']
    
    # Claims ç´¢å¼•
    claims_by_doc = defaultdict(list)
    for claim in data_tables['claims']:
        claims_by_doc[claim['doc_id']].append(claim)
    
    # 3. åˆ›å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡
    contexts = []
    
    print("ğŸš€ åˆ›å»ºå¢å¼ºä¸Šä¸‹æ–‡...")
    
    # 3.1 ä¸»æ–‡æ¡£æ®µè½ï¼ˆå¢å¼ºç‰ˆï¼‰
    for passage in data_tables['passages']:
        doc_id = passage['doc_id']
        doc_info = docs_by_id.get(doc_id, {})
        
        # æ‰¾åˆ°ç›¸å…³çš„å¥å­
        passage_sentences = [
            s for s in sentences_by_doc[doc_id]
            if (s.get('start_char', 0) >= passage.get('start_char', 0) and 
                s.get('end_char', 0) <= passage.get('end_char', 0))
        ]
        
        # æ‰¾åˆ°ç›¸å…³çš„å¼•ç”¨æåŠ
        passage_ref_mentions = [
            rm for rm in ref_mentions_by_doc[doc_id]
            if any(s.get('global_sent_id') == rm.get('sent_idx') for s in passage_sentences)
        ]
        
        # æ‰¾åˆ°ç›¸å…³çš„ç»´åŸºé“¾æ¥
        passage_wikilinks = [
            wl for wl in wikilinks_by_doc[doc_id]
            if (wl.get('anchor_start_char', 0) >= passage.get('start_char', 0) and 
                wl.get('anchor_end_char', 0) <= passage.get('end_char', 0))
        ]
        
        # æ‰¾åˆ°ç›¸å…³çš„claims
        passage_claims = [
            c for c in claims_by_doc[doc_id]
            if any(s.get('global_sent_id') == c.get('sent_idx') for s in passage_sentences)
        ]
        
        # æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡
        enhanced_text = passage['text']
        
        # å¯é€‰ï¼šæ·»åŠ å¼•ç”¨ä¿¡æ¯åˆ°æ–‡æœ¬ä¸­
        if passage_ref_mentions:
            ref_info = []
            for rm in passage_ref_mentions:
                ref_id = rm['ref_id']
                # æ‰¾åˆ°å¯¹åº”çš„å¼•ç”¨è¯¦æƒ…
                ref_detail = next((r for r in refs_by_doc[doc_id] if r['ref_id'] == ref_id), None)
                if ref_detail and ref_detail.get('title'):
                    ref_info.append(f"[{ref_id}: {ref_detail['title']}]")
            
            if ref_info:
                enhanced_text += f"\n\nReferences: {'; '.join(ref_info)}"
        
        # å¯é€‰ï¼šæ·»åŠ ç»´åŸºé“¾æ¥ä¿¡æ¯
        if passage_wikilinks:
            entity_info = [f"{wl['anchor_text']} -> {wl['target_title']}" for wl in passage_wikilinks]
            if entity_info:
                enhanced_text += f"\n\nLinked Entities: {'; '.join(entity_info)}"
        
        context = {
            "context": enhanced_text,
            "id": passage['passage_id'],
            "type": "main_passage",
            "metadata": {
                "doc_id": doc_id,
                "doc_title": doc_info.get('title'),
                "section_id": passage.get('section_id'),
                "n_tokens": passage.get('n_tokens'),
                "n_sentences": len(passage_sentences),
                "n_references": len(passage_ref_mentions),
                "n_wikilinks": len(passage_wikilinks),
                "n_claims": len(passage_claims),
                "reference_ids": [rm['ref_id'] for rm in passage_ref_mentions],
                "linked_entities": [wl['target_title'] for wl in passage_wikilinks],
                "has_claims": len(passage_claims) > 0
            }
        }
        contexts.append(context)
    
    # 3.2 å¤–éƒ¨æ–‡æ¡£æ®µè½ï¼ˆå¢å¼ºç‰ˆï¼‰
    for ext_passage in data_tables['ext_passages']:
        ext_doc_id = ext_passage['ext_doc_id']
        ext_doc_info = ext_docs_by_id.get(ext_doc_id, {})
        
        # æ‰¾åˆ°å¼•ç”¨è¿™ä¸ªå¤–éƒ¨æ–‡æ¡£çš„ä¸»æ–‡æ¡£
        referencing_refs = [ref_id for ref_id, ext_id in ext_docs_by_ref.items() if ext_id == ext_doc_id]
        
        # æ„å»ºå¤–éƒ¨æ–‡æ¡£çš„å¢å¼ºæ–‡æœ¬
        enhanced_text = ext_passage['text']
        
        # æ·»åŠ å¤–éƒ¨æ–‡æ¡£çš„å…ƒä¿¡æ¯
        if ext_doc_info.get('title'):
            enhanced_text = f"Source: {ext_doc_info['title']}\n\n{enhanced_text}"
        
        if ext_doc_info.get('source'):
            enhanced_text += f"\n\nPublished by: {ext_doc_info['source']}"
        
        context = {
            "context": enhanced_text,
            "id": ext_passage['ext_passage_id'],
            "type": "external_passage",
            "metadata": {
                "ext_doc_id": ext_doc_id,
                "title": ext_doc_info.get('title'),
                "source": ext_doc_info.get('source'),
                "source_type": ext_doc_info.get('source_type'),
                "url": ext_doc_info.get('url'),
                "authors": ext_doc_info.get('authors'),
                "publish_date": ext_doc_info.get('publish_date'),
                "n_tokens": ext_passage.get('n_tokens'),
                "referenced_by": referencing_refs,
                "has_fulltext": ext_doc_info.get('has_fulltext', False)
            }
        }
        contexts.append(context)
    
    # 3.3 Claimsï¼ˆä½œä¸ºç‹¬ç«‹çš„ä¸Šä¸‹æ–‡ï¼‰
    for claim in data_tables['claims']:
        doc_id = claim['doc_id']
        doc_info = docs_by_id.get(doc_id, {})
        
        # æ‰¾åˆ°ç›¸å…³çš„å¤–éƒ¨æ–‡æ¡£ä¿¡æ¯
        ext_doc_id = claim.get('ext_doc_id')
        ext_doc_info = ext_docs_by_id.get(ext_doc_id, {}) if ext_doc_id else {}
        
        enhanced_text = claim['claim_text']
        
        # æ·»åŠ æ”¯æ’‘è¯æ®ä¿¡æ¯
        if ext_doc_info.get('title'):
            enhanced_text += f"\n\nSupported by: {ext_doc_info['title']}"
            if ext_doc_info.get('source'):
                enhanced_text += f" ({ext_doc_info['source']})"
        
        context = {
            "context": enhanced_text,
            "id": claim['claim_id'],
            "type": "claim",
            "metadata": {
                "doc_id": doc_id,
                "doc_title": doc_info.get('title'),
                "ref_id": claim.get('ref_id'),
                "ext_doc_id": ext_doc_id,
                "ext_passage_id": claim.get('ext_passage_id'),
                "label": claim.get('label'),
                "sent_idx": claim.get('sent_idx'),
                "has_evidence": bool(ext_doc_id)
            }
        }
        contexts.append(context)
    
    # 3.4 æ–‡æ¡£çº§åˆ«çš„ä¸Šä¸‹æ–‡ï¼ˆåŸºäºsectionsï¼‰
    for doc_id, doc_sections in sections_by_doc.items():
        doc_info = docs_by_id.get(doc_id, {})
        
        # ä¸ºæ¯ä¸ªä¸»è¦ç« èŠ‚åˆ›å»ºæ¦‚è§ˆä¸Šä¸‹æ–‡
        for section in doc_sections:
            if section.get('level', 1) <= 2:  # åªå¤„ç†1-2çº§æ ‡é¢˜
                section_sentences = sentences_by_section.get(section['section_id'], [])
                section_text = ' '.join([s['text'] for s in section_sentences[:3]])  # å–å‰3å¥ä½œä¸ºæ¦‚è§ˆ
                
                if len(section_text.strip()) > 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å®¹
                    context = {
                        "context": f"Section: {section['heading']}\n\n{section_text}...",
                        "id": f"section_{section['section_id']}",
                        "type": "section_overview",
                        "metadata": {
                            "doc_id": doc_id,
                            "doc_title": doc_info.get('title'),
                            "section_id": section['section_id'],
                            "heading": section['heading'],
                            "level": section['level'],
                            "n_sentences": len(section_sentences)
                        }
                    }
                    contexts.append(context)
    
    print(f"  âœ… åˆ›å»ºäº† {len(contexts)} ä¸ªå¢å¼ºä¸Šä¸‹æ–‡")
    
    # ç»Ÿè®¡ä¿¡æ¯
    type_counts = defaultdict(int)
    for ctx in contexts:
        type_counts[ctx['type']] += 1
    
    print("ğŸ“Š ä¸Šä¸‹æ–‡ç±»å‹ç»Ÿè®¡:")
    for ctx_type, count in type_counts.items():
        print(f"  {ctx_type}: {count}")
    
    return contexts


def extract_unique_contexts(input_directory, output_directory):
    """å¤„ç† UltraWikiDomain corpus æ•°æ®"""
    os.makedirs(output_directory, exist_ok=True)
    
    print(f"ğŸš€ å¤„ç† UltraWikiDomain corpus: {input_directory}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ corpus ç›®å½•
    required_files = ['passages.jsonl', 'ext_passages.jsonl', 'docs.jsonl']
    missing_files = []
    for file_name in required_files:
        if not os.path.exists(os.path.join(input_directory, file_name)):
            missing_files.append(file_name)
    
    if missing_files:
        print(f"âŒ ç¼ºå°‘å¿…éœ€æ–‡ä»¶: {missing_files}")
        print("è¯·ç¡®ä¿è¾“å…¥ç›®å½•æ˜¯ UltraWikiDomain corpus ç›®å½•")
        return
    
    # åˆ›å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡
    contexts = create_enhanced_contexts(input_directory)
    
    if not contexts:
        print("âŒ æ²¡æœ‰åˆ›å»ºä»»ä½•ä¸Šä¸‹æ–‡")
        return
    
    # å»é‡
    unique_contexts_dict = {}
    for ctx in contexts:
        context_text = ctx['context']
        if context_text and context_text not in unique_contexts_dict:
            unique_contexts_dict[context_text] = ctx
    
    unique_contexts_list = list(unique_contexts_dict.keys())
    unique_contexts_with_metadata = list(unique_contexts_dict.values())
    
    print(f"ğŸ“Š å»é‡å: {len(unique_contexts_list)} ä¸ªå”¯ä¸€ä¸Šä¸‹æ–‡")
    
    # ä¿å­˜ç»“æœ
    CLASS = os.getenv("CLASS", "ultrawiki")
    output_filename = f"{CLASS}_unique_contexts.json"
    output_path = os.path.join(output_directory, output_filename)
    
    try:
        with open(output_path, "w", encoding="utf-8") as outfile:
            json.dump(unique_contexts_list, outfile, ensure_ascii=False, indent=2)
        print(f"âœ… å”¯ä¸€ä¸Šä¸‹æ–‡å·²ä¿å­˜åˆ°: {output_filename}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return
    
    # ä¿å­˜å¸¦å…ƒæ•°æ®çš„å®Œæ•´ç‰ˆæœ¬
    metadata_filename = "ultrawiki_contexts_with_metadata.json"
    metadata_path = os.path.join(output_directory, metadata_filename)
    
    try:
        with open(metadata_path, "w", encoding="utf-8") as outfile:
            json.dump(unique_contexts_with_metadata, outfile, ensure_ascii=False, indent=2)
        print(f"âœ… å¸¦å…ƒæ•°æ®çš„ä¸Šä¸‹æ–‡å·²ä¿å­˜åˆ°: {metadata_filename}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    # åˆ›å»º LightRAG éœ€è¦çš„æ ¼å¼
    lightrag_filename = "ultrawiki.jsonl"
    lightrag_path = os.path.join(output_directory, lightrag_filename)
    
    try:
        with open(lightrag_path, "w", encoding="utf-8") as outfile:
            for ctx in unique_contexts_with_metadata:
                lightrag_record = {
                    "context": ctx["context"],
                    "id": ctx["id"],
                    "type": ctx["type"],
                    "metadata": ctx["metadata"]
                }
                outfile.write(json.dumps(lightrag_record, ensure_ascii=False) + '\n')
        print(f"âœ… LightRAGæ ¼å¼å·²ä¿å­˜åˆ°: {lightrag_filename}")
    except Exception as e:
        print(f"âŒ ä¿å­˜LightRAGæ ¼å¼æ—¶å‡ºé”™: {e}")
    
    print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ!")


if __name__ == "__main__":
    CLASS = os.getenv("CLASS", "ultrawiki")
    parser = argparse.ArgumentParser(description="å¤„ç† UltraWikiDomain corpus æ•°æ®")
    parser.add_argument(
        "-i", "--input_dir", 
        type=str, 
        default="/mnt/jfs/wangpengyu/UltraWikiDomain/corpus",
        help="UltraWikiDomain corpus ç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "-o", "--output_dir", 
        type=str, 
        default="../exp_results/data/unique_contexts",
        help="è¾“å‡ºç›®å½•è·¯å¾„"
    )

    args = parser.parse_args()

    extract_unique_contexts(args.input_dir, args.output_dir)