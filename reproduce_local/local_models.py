#!/usr/bin/env python3
"""
æœ¬åœ°æ¨¡å‹é…ç½®æ–‡ä»¶
OSS API + Qwen3-Embedding-0.6B
"""

import requests
import json
import torch
import numpy as np
import os
from transformers import AutoTokenizer, AutoModel
from typing import List, Any, Dict
import asyncio
import aiohttp
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# ==================== OSS LLM é…ç½® ====================
import threading
import random

# OSS LLMè´Ÿè½½å‡è¡¡é…ç½®
_oss_counter = 0
_oss_lock = threading.Lock()

# ==================== EmbeddingæœåŠ¡é…ç½® ====================
# EmbeddingæœåŠ¡è´Ÿè½½å‡è¡¡é…ç½®
_embedding_counter = 0
_embedding_lock = threading.Lock()
_use_remote_embedding = False  # æ˜¯å¦ä½¿ç”¨è¿œç¨‹EmbeddingæœåŠ¡

def get_oss_config():
    """è·å–OSSé…ç½®ï¼Œæ”¯æŒå¤šç«¯å£è´Ÿè½½å‡è¡¡"""
    global _oss_counter
    
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    oss_host = os.getenv("OSS_HOST", "10.0.4.178")
    oss_ports = os.getenv("OSS_PORTS", "30066")
    
    # è§£æç«¯å£åˆ—è¡¨
    if "," in oss_ports:
        ports_list = [port.strip() for port in oss_ports.split(",")]
    else:
        ports_list = [oss_ports]
    
    # è½®è¯¢é€‰æ‹©ç«¯å£
    with _oss_lock:
        port = ports_list[_oss_counter % len(ports_list)]
        _oss_counter += 1
    
    return {
        "url": f"http://{oss_host}:{port}/v1/chat/completions",
        "headers": {
            "Content-Type": "application/json",
            "Authorization": "Bearer dummy_token"
        },
        "model": "default",
        "port": port,
        "host": oss_host
    }

def get_embedding_config():
    """è·å–Embeddingé…ç½®ï¼Œæ”¯æŒå¤šç«¯å£è´Ÿè½½å‡è¡¡"""
    global _embedding_counter
    
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    embedding_host = os.getenv("EMBEDDING_HOST", "10.0.4.178")
    embedding_ports = os.getenv("EMBEDDING_PORTS", "30151")
    
    # è§£æç«¯å£åˆ—è¡¨
    if "," in embedding_ports:
        ports_list = [port.strip() for port in embedding_ports.split(",")]
    else:
        ports_list = [embedding_ports]
    
    # è½®è¯¢é€‰æ‹©ç«¯å£
    with _embedding_lock:
        port = ports_list[_embedding_counter % len(ports_list)]
        _embedding_counter += 1
    
    return {
        "url": f"http://{embedding_host}:{port}/v1/embeddings",
        "headers": {
            "Content-Type": "application/json"
        },
        "port": port,
        "host": embedding_host
    }

def enable_remote_embedding():
    """å¯ç”¨è¿œç¨‹EmbeddingæœåŠ¡"""
    global _use_remote_embedding
    _use_remote_embedding = True
    print("âœ… å·²å¯ç”¨è¿œç¨‹EmbeddingæœåŠ¡")

def disable_remote_embedding():
    """ç¦ç”¨è¿œç¨‹EmbeddingæœåŠ¡ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹"""
    global _use_remote_embedding
    _use_remote_embedding = False
    print("âœ… å·²åˆ‡æ¢åˆ°æœ¬åœ°Embeddingæ¨¡å‹")

# ==================== Qwen Embedding é…ç½® ====================
QWEN_MODEL_PATH = "/mnt/jfs/xubenfeng/rag/models_and_datasets/Qwen3-Embedding-0.6B"

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
_tokenizer = None
_model = None
_device = None

def filter_json_serializable_kwargs(kwargs: Dict[str, Any]) -> Dict[str, Any]:
    """è¿‡æ»¤å‡ºå¯ä»¥JSONåºåˆ—åŒ–çš„å‚æ•°"""
    filtered_kwargs = {}
    
    # åªä¿ç•™è¿™äº›ç±»å‹çš„å‚æ•°ï¼Œè¿™äº›æ˜¯OpenAI APIå¯èƒ½éœ€è¦çš„
    allowed_params = {
        'temperature', 'max_tokens', 'top_p', 'frequency_penalty', 
        'presence_penalty', 'stop', 'stream', 'logit_bias', 'user',
        'seed', 'top_logprobs', 'logprobs'
    }
    
    for key, value in kwargs.items():
        # åªå¤„ç†å…è®¸çš„å‚æ•°å
        if key not in allowed_params:
            # print(f"ğŸ”§ è·³è¿‡éAPIå‚æ•°: {key} = {type(value)}")
            continue
            
        # æ£€æŸ¥å€¼çš„ç±»å‹
        if isinstance(value, (str, int, float, bool, type(None))):
            filtered_kwargs[key] = value
        elif isinstance(value, (list, dict)):
            try:
                # å°è¯•åºåˆ—åŒ–å¤æ‚ç±»å‹
                json.dumps(value)
                filtered_kwargs[key] = value
            except (TypeError, ValueError):
                print(f"ğŸ”§ è·³è¿‡ä¸èƒ½åºåˆ—åŒ–çš„å‚æ•°: {key} = {type(value)}")
        else:
            print(f"ğŸ”§ è·³è¿‡ä¸æ”¯æŒçš„ç±»å‹: {key} = {type(value)}")
    
    return filtered_kwargs

def init_qwen_embedding():
    """åˆå§‹åŒ–Qwen Embeddingæ¨¡å‹"""
    global _tokenizer, _model, _device
    
    if _tokenizer is None:
        print("æ­£åœ¨åŠ è½½Qwen3-Embedding-0.6Bæ¨¡å‹...")
        _tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_PATH, trust_remote_code=True)
        _model = AutoModel.from_pretrained(QWEN_MODEL_PATH, trust_remote_code=True)
        _model.eval()
        
        _device = "cuda" if torch.cuda.is_available() else "cpu"
        _model = _model.to(_device)
        print(f"âœ… Qwen Embeddingæ¨¡å‹åŠ è½½å®Œæˆ! è®¾å¤‡: {_device}")

def oss_llm_complete(
    prompt, 
    system_prompt=None, 
    history_messages=[], 
    model="default",
    max_tokens=8192,
    temperature=0.7,
    **kwargs
) -> str:
    """
    OSS LLMåŒæ­¥è°ƒç”¨å‡½æ•°ï¼Œæ”¯æŒè´Ÿè½½å‡è¡¡
    """
    # è·å–è´Ÿè½½å‡è¡¡çš„é…ç½®
    oss_config = get_oss_config()
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    # æ·»åŠ å†å²æ¶ˆæ¯
    messages.extend(history_messages)
    messages.append({"role": "user", "content": prompt})
    
    # è¿‡æ»¤å¯åºåˆ—åŒ–çš„kwargs
    filtered_kwargs = filter_json_serializable_kwargs(kwargs)
    
    data = {
        "model": oss_config["model"],
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        **filtered_kwargs
    }
    
    try:
        response = requests.post(
            oss_config["url"], 
            headers=oss_config["headers"], 
            json=data, 
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            
            # å¤„ç†thinkingæ¨¡å¼çš„è¾“å‡º
            if "analysis" in content and "assistantfinal" in content:
                assistantfinal_start = content.find("assistantfinal")
                if assistantfinal_start != -1:
                    content = content[assistantfinal_start:].replace("assistantfinal", "").strip()
            
            # è¾“å‡ºè´Ÿè½½å‡è¡¡ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œè°ƒè¯•æ—¶ä½¿ç”¨ï¼‰
            # print(f"ğŸ”„ ä½¿ç”¨OSSæœåŠ¡: {oss_config['host']}:{oss_config['port']}")
            
            return content
        else:
            print(f"âŒ OSS APIè¯·æ±‚å¤±è´¥! æœåŠ¡: {oss_config['host']}:{oss_config['port']}, çŠ¶æ€ç : {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            return f"Error: {response.status_code} - {response.text}"
            
    except Exception as e:
        print(f"âŒ OSS APIè°ƒç”¨å¼‚å¸¸: æœåŠ¡: {oss_config['host']}:{oss_config['port']}, é”™è¯¯: {e}")
        return f"Error: {str(e)}"

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((
        asyncio.TimeoutError,
        aiohttp.ClientError,
        ConnectionError,
        OSError
    ))
)
async def oss_llm_complete_async(
    prompt, 
    system_prompt=None, 
    history_messages=[], 
    model="default",
    max_tokens=8192,
    temperature=0.7,
    **kwargs
) -> str:
    """
    OSS LLMå¼‚æ­¥è°ƒç”¨å‡½æ•°ï¼Œæ”¯æŒè´Ÿè½½å‡è¡¡
    """
    # è·å–è´Ÿè½½å‡è¡¡çš„é…ç½®
    oss_config = get_oss_config()
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    messages.extend(history_messages)
    messages.append({"role": "user", "content": prompt})
    
    # è¿‡æ»¤å¯åºåˆ—åŒ–çš„kwargs
    filtered_kwargs = filter_json_serializable_kwargs(kwargs)
    
    data = {
        "model": oss_config["model"],
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        **filtered_kwargs
    }
    
    try:
        # åˆ›å»ºè¿æ¥å™¨ï¼Œé™åˆ¶å¹¶å‘è¿æ¥æ•°
        connector = aiohttp.TCPConnector(limit=10, limit_per_host=5)
        async with aiohttp.ClientSession(connector=connector) as session:
            async with session.post(
                oss_config["url"], 
                headers=oss_config["headers"], 
                json=data,
                timeout=aiohttp.ClientTimeout(total=600)  # å¢åŠ è¶…æ—¶æ—¶é—´
            ) as response:
                
                if response.status == 200:
                    result = await response.json()
                    content = result["choices"][0]["message"]["content"]
                    
                    # å¤„ç†thinkingæ¨¡å¼çš„è¾“å‡º
                    if "analysis" in content and "assistantfinal" in content:
                        assistantfinal_start = content.find("assistantfinal")
                        if assistantfinal_start != -1:
                            content = content[assistantfinal_start:].replace("assistantfinal", "").strip()
                    
                    # è¾“å‡ºè´Ÿè½½å‡è¡¡ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œè°ƒè¯•æ—¶ä½¿ç”¨ï¼‰
                    # print(f"ğŸ”„ ä½¿ç”¨OSSæœåŠ¡: {oss_config['host']}:{oss_config['port']}")
                    
                    return content
                else:
                    error_text = await response.text()
                    print(f"âŒ OSS APIè¯·æ±‚å¤±è´¥! æœåŠ¡: {oss_config['host']}:{oss_config['port']}, çŠ¶æ€ç : {response.status}")
                    print(f"é”™è¯¯ä¿¡æ¯: {error_text}")
                    return f"Error: {response.status} - {error_text}"
                    
    except asyncio.TimeoutError:
        print(f"â° OSS APIè¶…æ—¶: æœåŠ¡: {oss_config['host']}:{oss_config['port']} (120ç§’)")
        return "Error: Timeout after 120 seconds"
    except aiohttp.ClientError as e:
        print(f"ğŸŒ OSS APIè¿æ¥é”™è¯¯: æœåŠ¡: {oss_config['host']}:{oss_config['port']}, é”™è¯¯: {e}")
        return f"Error: Connection error - {str(e)}"
    except Exception as e:
        print(f"âŒ OSS APIå¼‚æ­¥è°ƒç”¨å¼‚å¸¸: æœåŠ¡: {oss_config['host']}:{oss_config['port']}, é”™è¯¯ç±»å‹: {type(e).__name__}, è¯¦æƒ…: {e}")
        return f"Error: {type(e).__name__} - {str(e)}"

# ==================== è¿œç¨‹Embeddingè°ƒç”¨å‡½æ•° ====================

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=5),
    retry=retry_if_exception_type((
        asyncio.TimeoutError,
        aiohttp.ClientError,
        ConnectionError,
        OSError
    ))
)
async def remote_embedding_async(texts: List[str]) -> np.ndarray:
    """è¿œç¨‹Embeddingå¼‚æ­¥è°ƒç”¨å‡½æ•°ï¼Œæ”¯æŒè´Ÿè½½å‡è¡¡"""
    embedding_config = get_embedding_config()
    
    data = {
        "input": texts,
        "model": "qwen-embedding"
    }
    
    try:
        # åˆ›å»ºè¿æ¥å™¨ï¼Œé™åˆ¶å¹¶å‘è¿æ¥æ•°
        connector = aiohttp.TCPConnector(limit=20, limit_per_host=10)
        async with aiohttp.ClientSession(connector=connector) as session:
            async with session.post(
                embedding_config["url"],
                headers=embedding_config["headers"],
                json=data,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                
                if response.status == 200:
                    result = await response.json()
                    embeddings = []
                    for item in result["data"]:
                        embeddings.append(item["embedding"])
                    return np.array(embeddings, dtype=np.float32)
                else:
                    error_text = await response.text()
                    print(f"âŒ Embedding APIè¯·æ±‚å¤±è´¥! æœåŠ¡: {embedding_config['host']}:{embedding_config['port']}, çŠ¶æ€ç : {response.status}")
                    print(f"é”™è¯¯ä¿¡æ¯: {error_text}")
                    raise RuntimeError(f"Embedding API Error: {response.status} - {error_text}")
                    
    except asyncio.TimeoutError:
        print(f"â° Embedding APIè¶…æ—¶: æœåŠ¡: {embedding_config['host']}:{embedding_config['port']} (60ç§’)")
        raise
    except aiohttp.ClientError as e:
        print(f"ğŸŒ Embedding APIè¿æ¥é”™è¯¯: æœåŠ¡: {embedding_config['host']}:{embedding_config['port']}, é”™è¯¯: {e}")
        raise
    except Exception as e:
        print(f"âŒ Embedding APIå¼‚æ­¥è°ƒç”¨å¼‚å¸¸: æœåŠ¡: {embedding_config['host']}:{embedding_config['port']}, é”™è¯¯ç±»å‹: {type(e).__name__}, è¯¦æƒ…: {e}")
        raise

def remote_embedding(texts: List[str]) -> np.ndarray:
    """è¿œç¨‹EmbeddingåŒæ­¥è°ƒç”¨å‡½æ•°"""
    embedding_config = get_embedding_config()
    
    data = {
        "input": texts,
        "model": "qwen-embedding"
    }
    
    try:
        response = requests.post(
            embedding_config["url"],
            headers=embedding_config["headers"],
            json=data,
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            embeddings = []
            for item in result["data"]:
                embeddings.append(item["embedding"])
            return np.array(embeddings, dtype=np.float32)
        else:
            print(f"âŒ Embedding APIè¯·æ±‚å¤±è´¥! æœåŠ¡: {embedding_config['host']}:{embedding_config['port']}, çŠ¶æ€ç : {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            raise RuntimeError(f"Embedding API Error: {response.status_code} - {response.text}")
            
    except Exception as e:
        print(f"âŒ Embedding APIè°ƒç”¨å¼‚å¸¸: æœåŠ¡: {embedding_config['host']}:{embedding_config['port']}, é”™è¯¯: {e}")
        raise e

def qwen_embedding(texts: List[str]) -> np.ndarray:
    """
    Qwen EmbeddingåŒæ­¥è°ƒç”¨å‡½æ•° - æ”¯æŒè¿œç¨‹/æœ¬åœ°åˆ‡æ¢
    """
    global _use_remote_embedding, _tokenizer, _model, _device
    
    # å¦‚æœå¯ç”¨äº†è¿œç¨‹æœåŠ¡ï¼Œä½¿ç”¨è¿œç¨‹è°ƒç”¨
    if _use_remote_embedding:
        return remote_embedding(texts)
    
    # æœ¬åœ°æ¨¡å¼ï¼šç¡®ä¿æ¨¡å‹å·²åŠ è½½
    if _tokenizer is None:
        init_qwen_embedding()
    
    try:
        # Tokenizeè¾“å…¥æ–‡æœ¬
        inputs = _tokenizer(
            texts, 
            padding=True, 
            truncation=True, 
            return_tensors="pt", 
            max_length=512
        )
        inputs = {k: v.to(_device) for k, v in inputs.items()}
        
        # ç”Ÿæˆembeddings
        with torch.no_grad():
            outputs = _model(**inputs)
            # ä½¿ç”¨mean pooling
            embeddings = outputs.last_hidden_state.mean(dim=1)
            embeddings = embeddings.cpu().numpy()
        
        return embeddings
        
    except Exception as e:
        print(f"âŒ Qwen Embeddingç”Ÿæˆå¤±è´¥: {e}")
        raise e

async def qwen_embedding_async(texts: List[str]) -> np.ndarray:
    """
    Qwen Embeddingå¼‚æ­¥è°ƒç”¨å‡½æ•° - æ”¯æŒè¿œç¨‹/æœ¬åœ°åˆ‡æ¢
    """
    global _use_remote_embedding
    
    # å¦‚æœå¯ç”¨äº†è¿œç¨‹æœåŠ¡ï¼Œä½¿ç”¨è¿œç¨‹å¼‚æ­¥è°ƒç”¨
    if _use_remote_embedding:
        return await remote_embedding_async(texts)
    
    # æœ¬åœ°æ¨¡å¼ï¼šå¼‚æ­¥æ‰§è¡Œæœ¬åœ°embedding
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, qwen_embedding, texts)

# ==================== LightRAG å…¼å®¹å‡½æ•° ====================

def lightrag_llm_func(
    prompt, 
    system_prompt=None, 
    history_messages=[], 
    keyword_extraction=False,
    **kwargs
) -> str:
    """
    LightRAGå…¼å®¹çš„LLMå‡½æ•°
    """
    return oss_llm_complete(
        prompt=prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        **kwargs
    )

async def lightrag_llm_func_async(
    prompt, 
    system_prompt=None, 
    history_messages=[], 
    keyword_extraction=False,
    **kwargs
) -> str:
    """
    LightRAGå…¼å®¹çš„å¼‚æ­¥LLMå‡½æ•°
    """
    return await oss_llm_complete_async(
        prompt=prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        **kwargs
    )

def lightrag_embedding_func(texts: List[str]) -> np.ndarray:
    """
    LightRAGå…¼å®¹çš„Embeddingå‡½æ•°
    """
    return qwen_embedding(texts)

async def lightrag_embedding_func_async(texts: List[str]) -> np.ndarray:
    """
    LightRAGå…¼å®¹çš„å¼‚æ­¥Embeddingå‡½æ•°
    """
    return await qwen_embedding_async(texts)

# ==================== è·å–Embeddingç»´åº¦ ====================
def get_embedding_dim() -> int:
    """è·å–Embeddingç»´åº¦"""
    # ç”¨ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬æ¥è·å–ç»´åº¦
    test_embedding = qwen_embedding(["test"])
    return test_embedding.shape[1]

def show_oss_config():
    """æ˜¾ç¤ºå½“å‰OSSè´Ÿè½½å‡è¡¡é…ç½®"""
    oss_host = os.getenv("OSS_HOST", "10.0.4.178")
    oss_ports = os.getenv("OSS_PORTS", "30066")
    
    if "," in oss_ports:
        ports_list = [port.strip() for port in oss_ports.split(",")]
        print(f"ğŸŒ OSSè´Ÿè½½å‡è¡¡é…ç½®:")
        print(f"  ä¸»æœº: {oss_host}")
        print(f"  ç«¯å£æ•°é‡: {len(ports_list)} ä¸ª")
        print(f"  ç«¯å£åˆ—è¡¨: {', '.join(ports_list)}")
        print(f"  è´Ÿè½½å‡è¡¡: è½®è¯¢ï¼ˆRound Robinï¼‰")
    else:
        print(f"ğŸŒ OSSå•ä¸€æœåŠ¡é…ç½®:")
        print(f"  æœåŠ¡åœ°å€: {oss_host}:{oss_ports}")

def show_embedding_config():
    """æ˜¾ç¤ºå½“å‰Embeddingè´Ÿè½½å‡è¡¡é…ç½®"""
    global _use_remote_embedding
    
    if _use_remote_embedding:
        embedding_host = os.getenv("EMBEDDING_HOST", "10.0.4.178")
        embedding_ports = os.getenv("EMBEDDING_PORTS", "30151")
        
        if "," in embedding_ports:
            ports_list = [port.strip() for port in embedding_ports.split(",")]
            print(f"ğŸ”® Embeddingè¿œç¨‹æœåŠ¡é…ç½®:")
            print(f"  ä¸»æœº: {embedding_host}")
            print(f"  ç«¯å£æ•°é‡: {len(ports_list)} ä¸ª")
            print(f"  ç«¯å£åˆ—è¡¨: {', '.join(ports_list)}")
            print(f"  è´Ÿè½½å‡è¡¡: è½®è¯¢ï¼ˆRound Robinï¼‰")
        else:
            print(f"ğŸ”® Embeddingå•ä¸€è¿œç¨‹æœåŠ¡é…ç½®:")
            print(f"  æœåŠ¡åœ°å€: {embedding_host}:{embedding_ports}")
    else:
        print(f"ğŸ”® Embeddingæœ¬åœ°æ¨¡å‹é…ç½®:")
        print(f"  æ¨¡å‹è·¯å¾„: {QWEN_MODEL_PATH}")
        print(f"  è¿è¡Œæ¨¡å¼: æœ¬åœ°GPU")

if __name__ == "__main__":
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    show_oss_config()
    
    # æµ‹è¯•ä»£ç 
    print("\næµ‹è¯•OSS LLM...")
    result = oss_llm_complete("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚")
    print(f"OSSå›å¤: {result[:100]}...")
    
    print("\næµ‹è¯•Qwen Embedding...")
    embeddings = qwen_embedding(["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•", "å¦ä¸€ä¸ªæµ‹è¯•"])
    print(f"Embeddingå½¢çŠ¶: {embeddings.shape}")
    print(f"Embeddingç»´åº¦: {get_embedding_dim()}")
