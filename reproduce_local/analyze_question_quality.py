import re
import json
import os
from collections import defaultdict

def analyze_question_granularity(questions_file, contexts_dir):
    """åˆ†æé—®é¢˜çš„ç²’åº¦å’Œç›¸å…³æ€§"""
    
    # è¯»å–ç”Ÿæˆçš„é—®é¢˜
    with open(questions_file, 'r', encoding='utf-8') as f:
        questions_text = f.read()
    
    questions = re.findall(r"- Question \d+: (.+)", questions_text)
    
    # åˆ†æé—®é¢˜ç‰¹å¾
    analysis = {
        "total_questions": len(questions),
        "macro_questions": 0,  # å®è§‚é—®é¢˜æ•°é‡
        "micro_questions": 0,  # ç»†ç²’åº¦é—®é¢˜æ•°é‡
        "keyword_distribution": defaultdict(int),
        "question_types": {
            "comparison": 0,
            "summary": 0, 
            "specific_fact": 0,
            "integration": 0
        }
    }
    
    # å®è§‚é—®é¢˜å…³é”®è¯
    macro_keywords = [
        "overall", "general", "main", "primary", "key", "important",
        "summary", "overview", "compare", "difference", "similarity",
        "framework", "approach", "methodology", "strategy"
    ]
    
    # ç»†ç²’åº¦é—®é¢˜å…³é”®è¯
    micro_keywords = [
        "specific", "detail", "exactly", "precisely", "particular",
        "step", "parameter", "value", "implementation", "code",
        "algorithm", "formula", "equation", "example"
    ]
    
    for question in questions:
        question_lower = question.lower()
        
        # ç»Ÿè®¡å…³é”®è¯
        macro_count = sum(1 for kw in macro_keywords if kw in question_lower)
        micro_count = sum(1 for kw in micro_keywords if kw in question_lower)
        
        if macro_count > micro_count:
            analysis["macro_questions"] += 1
        else:
            analysis["micro_questions"] += 1
            
        # é—®é¢˜ç±»å‹åˆ†ç±»
        if any(word in question_lower for word in ["compare", "difference", "versus"]):
            analysis["question_types"]["comparison"] += 1
        elif any(word in question_lower for word in ["summary", "overview", "main"]):
            analysis["question_types"]["summary"] += 1
        elif any(word in question_lower for word in ["how", "what", "which", "specific"]):
            analysis["question_types"]["specific_fact"] += 1
        elif any(word in question_lower for word in ["integrate", "combine", "relationship"]):
            analysis["question_types"]["integration"] += 1
    
    return analysis, questions

def generate_fine_grained_questions(contexts_dir, output_file):
    """ç”Ÿæˆç»†ç²’åº¦é—®é¢˜ï¼ŒåŸºäºå…·ä½“äº‹å®"""
    
    fine_grained_questions = []
    
    # éå†æ‰€æœ‰contextæ–‡ä»¶
    for filename in os.listdir(contexts_dir):
        if filename.endswith('.txt'):
            filepath = os.path.join(contexts_dir, filename)
            
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–å…·ä½“æ•°å­—ã€åç§°ã€å‚æ•°ç­‰
            numbers = re.findall(r'\d+\.?\d*', content)
            algorithms = re.findall(r'[A-Z][a-z]+(?:[A-Z][a-z]+)*(?=\s+algorithm|Algorithm)', content)
            parameters = re.findall(r'(\w+)\s*=\s*(\d+\.?\d*)', content)
            
            # åŸºäºå…·ä½“å†…å®¹ç”Ÿæˆé—®é¢˜
            if numbers:
                fine_grained_questions.append(f"What is the exact value of {numbers[0]} mentioned in {filename}?")
            
            if algorithms:
                fine_grained_questions.append(f"What are the specific steps of the {algorithms[0]} algorithm described in {filename}?")
                
            if parameters:
                param_name, param_value = parameters[0]
                fine_grained_questions.append(f"Why is {param_name} set to {param_value} in {filename}?")
    
    # ä¿å­˜ç»†ç²’åº¦é—®é¢˜
    with open(output_file, 'w', encoding='utf-8') as f:
        for i, question in enumerate(fine_grained_questions, 1):
            f.write(f"- Question {i}: {question}\n")
    
    return fine_grained_questions

if __name__ == "__main__":
    # åˆ†æç°æœ‰é—®é¢˜è´¨é‡
    questions_file = "../exp_results/data/questions/cs_questions.txt"
    contexts_dir = "../exp_results/data/unique_contexts"
    
    if os.path.exists(questions_file):
        analysis, questions = analyze_question_granularity(questions_file, contexts_dir)
        
        print("ğŸ“Š é—®é¢˜è´¨é‡åˆ†ææŠ¥å‘Š")
        print("=" * 50)
        print(f"æ€»é—®é¢˜æ•°: {analysis['total_questions']}")
        print(f"å®è§‚é—®é¢˜: {analysis['macro_questions']} ({analysis['macro_questions']/analysis['total_questions']*100:.1f}%)")
        print(f"ç»†ç²’åº¦é—®é¢˜: {analysis['micro_questions']} ({analysis['micro_questions']/analysis['total_questions']*100:.1f}%)")
        print("\né—®é¢˜ç±»å‹åˆ†å¸ƒ:")
        for qtype, count in analysis['question_types'].items():
            print(f"  {qtype}: {count}")
        
        # ç”Ÿæˆæ”¹è¿›çš„ç»†ç²’åº¦é—®é¢˜
        fine_grained_output = "../exp_results/data/questions/cs_fine_grained_questions.txt"
        fine_questions = generate_fine_grained_questions(contexts_dir, fine_grained_output)
        print(f"\nâœ… ç”Ÿæˆäº† {len(fine_questions)} ä¸ªç»†ç²’åº¦é—®é¢˜")
        print(f"ä¿å­˜åˆ°: {fine_grained_output}")