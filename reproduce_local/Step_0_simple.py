import os
import json
import argparse

def create_simple_contexts(corpus_directory):
    """åªä½¿ç”¨passages.jsonlåˆ›å»ºç®€å•ä¸Šä¸‹æ–‡ï¼Œé™åˆ¶æ•°é‡å’Œé•¿åº¦"""
    print(f"ğŸ“Š ä» {corpus_directory} åŠ è½½passagesæ•°æ®...")
    
    # åªåŠ è½½passages
    passages_file = os.path.join(corpus_directory, "passages.jsonl")
    if not os.path.exists(passages_file):
        print(f"âŒ æ‰¾ä¸åˆ° {passages_file}")
        return []
    
    contexts = []
    with open(passages_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                passage = json.loads(line)
                text = passage['text'].strip()
                
                # é™åˆ¶é•¿åº¦ï¼šåªå–å‰1000å­—ç¬¦ï¼Œé¿å…è¿‡é•¿
                if len(text) > 1000:
                    text = text[:1000] + "..."
                
                # è·³è¿‡å¤ªçŸ­çš„æ®µè½
                if len(text) > 100:
                    contexts.append(text)
    
    print(f"âœ… æå–äº† {len(contexts)} ä¸ªæ®µè½")
    
    # é™åˆ¶æ€»æ•°é‡ï¼Œåªå–å‰50ä¸ªæœ€é‡è¦çš„æ®µè½
    if len(contexts) > 50:
        print(f"âš ï¸ æ®µè½æ•°é‡è¿‡å¤š({len(contexts)})ï¼Œåªä¿ç•™å‰50ä¸ª")
        contexts = contexts[:50]
    
    return contexts

def main():
    parser = argparse.ArgumentParser(description="åˆ›å»ºç®€åŒ–çš„LightRAGä¸Šä¸‹æ–‡")
    parser.add_argument(
        "-i", "--input_dir", 
        type=str, 
        default="/mnt/jfs/wangpengyu/UltraWikiDomain/corpus",
        help="UltraWikiDomain corpus ç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "-o", "--output_dir", 
        type=str, 
        default="../exp_results/data/unique_contexts",
        help="è¾“å‡ºç›®å½•è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    corpus_dir = args.input_dir
    output_dir = args.output_dir
    
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆ›å»ºç®€å•ä¸Šä¸‹æ–‡
    contexts = create_simple_contexts(corpus_dir)
    
    if not contexts:
        print("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•ä¸Šä¸‹æ–‡")
        return
    
    # å»é‡
    unique_contexts = []
    seen = set()
    for ctx in contexts:
        if ctx not in seen:
            unique_contexts.append(ctx)
            seen.add(ctx)
    
    print(f"ğŸ“Š å»é‡å: {len(unique_contexts)} ä¸ªå”¯ä¸€æ®µè½")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_chars = sum(len(ctx) for ctx in unique_contexts)
    avg_chars = total_chars / len(unique_contexts) if unique_contexts else 0
    print(f"ğŸ“Š å¹³å‡é•¿åº¦: {avg_chars:.0f} å­—ç¬¦")
    print(f"ğŸ“Š æ€»å­—ç¬¦æ•°: {total_chars:,}")
    
    # ä¿å­˜
    CLASS = os.getenv("CLASS", "cs")
    output_file = os.path.join(output_dir, f"{CLASS}_unique_contexts.json")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(unique_contexts, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ¯ é¢„è®¡æ’å…¥æ—¶é—´: 2-5åˆ†é’Ÿ")

if __name__ == "__main__":
    main()