import asyncio
import json
import random
import os
import re
from local_models import lightrag_llm_func_async

class JudgeBiasAnalyzer:
    def __init__(self):
        self.results = {
            "position_bias": [],
            "length_bias": [],
            "trial_bias": [],  # è¯•æ¬¡åç½®
            "hybrid_vs_naive_comparison": []
        }
        self.questions = []
        self.hybrid_answers = {}
        self.naive_answers = {}
    
    def load_real_data(self, results_dir, questions_file):
        """åŠ è½½çœŸå®çš„é—®é¢˜å’Œç­”æ¡ˆæ•°æ®"""
        
        # åŠ è½½é—®é¢˜
        try:
            with open(questions_file, 'r', encoding='utf-8') as f:
                questions_text = f.read()
            
            # æå–é—®é¢˜
            self.questions = re.findall(r"- Question \d+: (.+)", questions_text)
            print(f"âœ… åŠ è½½äº† {len(self.questions)} ä¸ªé—®é¢˜")
            
        except FileNotFoundError:
            print(f"âŒ é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {questions_file}")
            return False
        
        # åªåŠ è½½hybridå’Œnaiveæ¨¡å¼çš„ç­”æ¡ˆ
        modes = ["naive", "hybrid"]
        
        for mode in modes:
            result_file = os.path.join(results_dir, f"cs_{mode}_results.json")
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if mode == "hybrid":
                    for item in data:
                        query = item.get("query", "")
                        result = item.get("result", "")
                        self.hybrid_answers[query] = result
                elif mode == "naive":
                    for item in data:
                        query = item.get("query", "")
                        result = item.get("result", "")
                        self.naive_answers[query] = result
                
                print(f"âœ… åŠ è½½äº† {mode} æ¨¡å¼çš„ {len(data)} ä¸ªç­”æ¡ˆ")
                
            except (FileNotFoundError, json.JSONDecodeError) as e:
                print(f"âŒ åŠ è½½ {mode} æ¨¡å¼ç»“æœå¤±è´¥: {e}")
        
        return len(self.questions) > 0 and len(self.hybrid_answers) > 0 and len(self.naive_answers) > 0
    
    def find_all_hybrid_naive_pairs(self):
        """æ‰¾åˆ°æ‰€æœ‰Hybridå’ŒNaiveçš„ç­”æ¡ˆå¯¹"""
        
        answer_pairs = []
        
        # éå†æ‰€æœ‰é—®é¢˜ï¼Œå¯»æ‰¾åœ¨ä¸¤ä¸ªæ¨¡å¼ä¸­éƒ½æœ‰ç­”æ¡ˆçš„é—®é¢˜
        for question in self.questions:
            hybrid_answer = None
            naive_answer = None
            
            # åœ¨hybridç­”æ¡ˆä¸­æŸ¥æ‰¾åŒ¹é…çš„é—®é¢˜
            for query, answer in self.hybrid_answers.items():
                if question in query or self._questions_match(question, query):
                    hybrid_answer = answer
                    break
            
            # åœ¨naiveç­”æ¡ˆä¸­æŸ¥æ‰¾åŒ¹é…çš„é—®é¢˜  
            for query, answer in self.naive_answers.items():
                if question in query or self._questions_match(question, query):
                    naive_answer = answer
                    break
            
            # å¦‚æœä¸¤ä¸ªæ¨¡å¼éƒ½æœ‰ç­”æ¡ˆï¼Œæ·»åŠ åˆ°ç­”æ¡ˆå¯¹ä¸­
            if hybrid_answer and naive_answer:
                length_diff = abs(len(hybrid_answer) - len(naive_answer))
                answer_pairs.append({
                    "question": question,
                    "hybrid_answer": hybrid_answer,
                    "naive_answer": naive_answer,
                    "length_diff": length_diff
                })
        
        print(f"âœ… æ‰¾åˆ° {len(answer_pairs)} ä¸ªHybrid vs Naiveç­”æ¡ˆå¯¹")
        return answer_pairs
    
    def _questions_match(self, q1, q2, threshold=0.8):
        """ç®€å•çš„é—®é¢˜åŒ¹é…ç®—æ³•"""
        # å°†é—®é¢˜è½¬æ¢ä¸ºå°å†™å¹¶å»é™¤æ ‡ç‚¹
        q1_clean = re.sub(r'[^\w\s]', '', q1.lower())
        q2_clean = re.sub(r'[^\w\s]', '', q2.lower())
        
        # è®¡ç®—è¯æ±‡é‡å ç‡
        words1 = set(q1_clean.split())
        words2 = set(q2_clean.split())
        
        if len(words1) == 0 or len(words2) == 0:
            return False
            
        overlap = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return overlap / union >= threshold
    
    async def test_position_bias_hybrid_naive(self, answer_pairs):
        """æµ‹è¯•Hybrid vs Naiveçš„ä½ç½®åç½®"""
        
        judge_prompt_template = """
è¯·è¯„åˆ¤ä»¥ä¸‹ä¸¤ä¸ªç­”æ¡ˆå“ªä¸ªæ›´å¥½åœ°å›ç­”äº†é—®é¢˜ã€‚è¯·ä»å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€æ¸…æ™°åº¦ç­‰æ–¹é¢è¿›è¡Œè¯„ä¼°ï¼š

é—®é¢˜ï¼š{question}

ç­”æ¡ˆAï¼š{answer_a}

ç­”æ¡ˆBï¼š{answer_b}

è¯·é€‰æ‹©æ›´å¥½çš„ç­”æ¡ˆï¼ˆAæˆ–Bï¼‰å¹¶ç®€è¦è¯´æ˜ç†ç”±ï¼š
"""
        
        position_bias_results = []
        
        print(f"å¼€å§‹æµ‹è¯•æ‰€æœ‰ {len(answer_pairs)} ä¸ªç­”æ¡ˆå¯¹çš„ä½ç½®åç½®...")
        
        for pair_idx, pair in enumerate(answer_pairs):
            print(f"\næµ‹è¯•ç­”æ¡ˆå¯¹ {pair_idx + 1}/{len(answer_pairs)}: {pair['question'][:100]}...")
            
            # æµ‹è¯• Hybrid(A) vs Naive(B)
            prompt_hybrid_first = judge_prompt_template.format(
                question=pair["question"],
                answer_a=pair["hybrid_answer"],
                answer_b=pair["naive_answer"]
            )
            
            # æµ‹è¯• Naive(A) vs Hybrid(B) - äº¤æ¢ä½ç½®
            prompt_naive_first = judge_prompt_template.format(
                question=pair["question"],
                answer_a=pair["naive_answer"],
                answer_b=pair["hybrid_answer"]
            )
            
            try:
                # æµ‹è¯•Hybridåœ¨Aä½ç½®
                result_hybrid_first = await lightrag_llm_func_async(
                    prompt_hybrid_first,
                    system="ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„è¯„åˆ¤è€…ï¼Œè¯·å®¢è§‚è¯„ä»·ç­”æ¡ˆè´¨é‡ã€‚",
                    max_tokens=300,
                    temperature=0.1
                )
                
                await asyncio.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
                # æµ‹è¯•Naiveåœ¨Aä½ç½®
                result_naive_first = await lightrag_llm_func_async(
                    prompt_naive_first,
                    system="ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„è¯„åˆ¤è€…ï¼Œè¯·å®¢è§‚è¯„ä»·ç­”æ¡ˆè´¨é‡ã€‚",
                    max_tokens=300,
                    temperature=0.1
                )
                
                # è§£æç»“æœ
                hybrid_first_choice = self.parse_judge_result(result_hybrid_first)
                naive_first_choice = self.parse_judge_result(result_naive_first)
                
                # è½¬æ¢ä¸ºå®é™…é€‰æ‹©çš„æ¨¡å¼
                if hybrid_first_choice == "A":
                    choice_when_hybrid_first = "hybrid"
                elif hybrid_first_choice == "B":
                    choice_when_hybrid_first = "naive"
                else:
                    choice_when_hybrid_first = "unclear"
                
                if naive_first_choice == "A":
                    choice_when_naive_first = "naive"
                elif naive_first_choice == "B":
                    choice_when_naive_first = "hybrid"
                else:
                    choice_when_naive_first = "unclear"
                
                # æ£€æŸ¥ä¸€è‡´æ€§
                consistent = choice_when_hybrid_first == choice_when_naive_first
                
                result_record = {
                    "pair_id": pair_idx,
                    "question": pair["question"][:100],
                    "choice_when_hybrid_first": choice_when_hybrid_first,
                    "choice_when_naive_first": choice_when_naive_first,
                    "consistent": consistent,
                    "length_diff": pair["length_diff"],
                    "hybrid_length": len(pair["hybrid_answer"]),
                    "naive_length": len(pair["naive_answer"])
                }
                
                position_bias_results.append(result_record)
                
                if not consistent:
                    print(f"ğŸš¨ ä½ç½®åç½®æ£€æµ‹: ç­”æ¡ˆå¯¹{pair_idx+1}")
                    print(f"  Hybridåœ¨å‰é€‰æ‹©: {choice_when_hybrid_first}")
                    print(f"  Naiveåœ¨å‰é€‰æ‹©: {choice_when_naive_first}")
                else:
                    print(f"âœ… ä¸€è‡´é€‰æ‹©: {choice_when_hybrid_first}")
                
                await asyncio.sleep(1)  # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                
            except Exception as e:
                print(f"âŒ ä½ç½®åç½®æµ‹è¯•å¤±è´¥ (ç­”æ¡ˆå¯¹{pair_idx+1}): {e}")
                await asyncio.sleep(2)
        
        self.results["position_bias"] = position_bias_results
        return position_bias_results
    
    async def test_length_bias_hybrid_naive(self, answer_pairs):
        """æµ‹è¯•Hybrid vs Naiveçš„é•¿åº¦åç½®"""
        
        judge_prompt_template = """
è¯·è¯„åˆ¤ä»¥ä¸‹ä¸¤ä¸ªç­”æ¡ˆå“ªä¸ªæ›´å¥½åœ°å›ç­”äº†é—®é¢˜ï¼š

é—®é¢˜ï¼š{question}

ç­”æ¡ˆAï¼š{answer_a}

ç­”æ¡ˆBï¼š{answer_b}

è¯·é€‰æ‹©æ›´å¥½çš„ç­”æ¡ˆï¼ˆAæˆ–Bï¼‰å¹¶ç®€è¦è¯´æ˜ç†ç”±ï¼š
"""
        
        length_bias_results = []
        
        # é€‰æ‹©é•¿åº¦å·®å¼‚è¾ƒå¤§çš„ç­”æ¡ˆå¯¹
        long_diff_pairs = [p for p in answer_pairs if p["length_diff"] >= 100]
        print(f"æµ‹è¯• {len(long_diff_pairs)} ä¸ªé•¿åº¦å·®å¼‚æ˜¾è‘—çš„ç­”æ¡ˆå¯¹...")
        
        for pair in long_diff_pairs:
            # ç¡®å®šé•¿çŸ­ç­”æ¡ˆ
            if len(pair["hybrid_answer"]) > len(pair["naive_answer"]):
                long_answer = pair["hybrid_answer"]
                short_answer = pair["naive_answer"]
                long_mode = "hybrid"
                short_mode = "naive"
            else:
                long_answer = pair["naive_answer"]
                short_answer = pair["hybrid_answer"]
                long_mode = "naive"
                short_mode = "hybrid"
            
            print(f"\næµ‹è¯•é•¿åº¦åç½®: {short_mode}({len(short_answer)}å­—) vs {long_mode}({len(long_answer)}å­—)")
            
            # æµ‹è¯•çŸ­ç­”æ¡ˆåœ¨å‰
            prompt_short_first = judge_prompt_template.format(
                question=pair["question"],
                answer_a=short_answer,
                answer_b=long_answer
            )
            
            # æµ‹è¯•é•¿ç­”æ¡ˆåœ¨å‰
            prompt_long_first = judge_prompt_template.format(
                question=pair["question"],
                answer_a=long_answer,
                answer_b=short_answer
            )
            
            try:
                # çŸ­ç­”æ¡ˆåœ¨å‰
                result_short_first = await lightrag_llm_func_async(
                    prompt_short_first,
                    system="ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„è¯„åˆ¤è€…ï¼Œè¯·å®¢è§‚è¯„ä»·ç­”æ¡ˆè´¨é‡ã€‚",
                    max_tokens=300,
                    temperature=0.1
                )
                
                await asyncio.sleep(1)
                
                # é•¿ç­”æ¡ˆåœ¨å‰
                result_long_first = await lightrag_llm_func_async(
                    prompt_long_first,
                    system="ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„è¯„åˆ¤è€…ï¼Œè¯·å®¢è§‚è¯„ä»·ç­”æ¡ˆè´¨é‡ã€‚",
                    max_tokens=300,
                    temperature=0.1
                )
                
                choice_short_first = self.parse_judge_result(result_short_first)
                choice_long_first = self.parse_judge_result(result_long_first)
                
                # åˆ¤æ–­æ˜¯å¦é€‰æ‹©äº†æ›´é•¿çš„ç­”æ¡ˆ
                chose_longer_when_short_first = choice_short_first == "B"
                chose_longer_when_long_first = choice_long_first == "A"
                
                length_bias_results.append({
                    "question": pair["question"][:100],
                    "short_mode": short_mode,
                    "long_mode": long_mode,
                    "chose_longer_when_short_first": chose_longer_when_short_first,
                    "chose_longer_when_long_first": chose_longer_when_long_first,
                    "length_ratio": len(long_answer) / len(short_answer),
                    "short_length": len(short_answer),
                    "long_length": len(long_answer)
                })
                
                print(f"  çŸ­åœ¨å‰é€‰æ‹©: {'é•¿ç­”æ¡ˆ' if chose_longer_when_short_first else 'çŸ­ç­”æ¡ˆ'}")
                print(f"  é•¿åœ¨å‰é€‰æ‹©: {'é•¿ç­”æ¡ˆ' if chose_longer_when_long_first else 'çŸ­ç­”æ¡ˆ'}")
                
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"âŒ é•¿åº¦åç½®æµ‹è¯•å¤±è´¥: {e}")
                await asyncio.sleep(2)
        
        self.results["length_bias"] = length_bias_results
        return length_bias_results
    
    async def test_trial_bias_hybrid_naive(self, answer_pairs):
        """æµ‹è¯•è¯•æ¬¡åç½®ï¼ˆå¤šæ¬¡è¯„åˆ¤åŒä¸€å¯¹ç­”æ¡ˆçš„ä¸€è‡´æ€§ï¼‰"""
        
        judge_prompt_template = """
è¯·è¯„åˆ¤ä»¥ä¸‹ä¸¤ä¸ªç­”æ¡ˆå“ªä¸ªæ›´å¥½åœ°å›ç­”äº†é—®é¢˜ã€‚è¯·ä»å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€æ¸…æ™°åº¦ç­‰æ–¹é¢è¿›è¡Œè¯„ä¼°ï¼š

é—®é¢˜ï¼š{question}

ç­”æ¡ˆAï¼š{answer_a}

ç­”æ¡ˆBï¼š{answer_b}

è¯·é€‰æ‹©æ›´å¥½çš„ç­”æ¡ˆï¼ˆAæˆ–Bï¼‰å¹¶ç®€è¦è¯´æ˜ç†ç”±ï¼š
"""
        
        trial_bias_results = []
        
        # é€‰æ‹©éƒ¨åˆ†ç­”æ¡ˆå¯¹è¿›è¡Œå¤šæ¬¡è¯•éªŒ
        test_pairs = random.sample(answer_pairs, min(5, len(answer_pairs)))  # é™åˆ¶æµ‹è¯•æ•°é‡
        trials_per_pair = 3  # æ¯å¯¹ç­”æ¡ˆæµ‹è¯•3æ¬¡
        
        print(f"å¼€å§‹æµ‹è¯•è¯•æ¬¡åç½®: {len(test_pairs)} ä¸ªç­”æ¡ˆå¯¹ï¼Œæ¯å¯¹æµ‹è¯• {trials_per_pair} æ¬¡...")
        
        for pair_idx, pair in enumerate(test_pairs):
            print(f"\næµ‹è¯•ç­”æ¡ˆå¯¹ {pair_idx + 1}/{len(test_pairs)}: {pair['question'][:100]}...")
            
            pair_results = {
                "pair_id": pair_idx,
                "question": pair["question"][:100],
                "hybrid_length": len(pair["hybrid_answer"]),
                "naive_length": len(pair["naive_answer"]),
                "trials": []
            }
            
            # è¿›è¡Œå¤šæ¬¡è¯•éªŒ
            for trial_num in range(trials_per_pair):
                print(f"  è¯•éªŒ {trial_num + 1}/{trials_per_pair}...")
                
                # éšæœºé€‰æ‹©ç­”æ¡ˆé¡ºåºï¼ˆé¿å…ä½ç½®åç½®å½±å“è¯•æ¬¡åç½®æµ‹è¯•ï¼‰
                if random.random() < 0.5:
                    # Hybridåœ¨å‰
                    prompt = judge_prompt_template.format(
                        question=pair["question"],
                        answer_a=pair["hybrid_answer"],
                        answer_b=pair["naive_answer"]
                    )
                    order = "hybrid_first"
                else:
                    # Naiveåœ¨å‰
                    prompt = judge_prompt_template.format(
                        question=pair["question"],
                        answer_a=pair["naive_answer"],
                        answer_b=pair["hybrid_answer"]
                    )
                    order = "naive_first"
                
                try:
                    result = await lightrag_llm_func_async(
                        prompt,
                        system="ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„è¯„åˆ¤è€…ï¼Œè¯·å®¢è§‚è¯„ä»·ç­”æ¡ˆè´¨é‡ã€‚",
                        max_tokens=300,
                        temperature=0.3  # ç¨å¾®æé«˜æ¸©åº¦ä»¥è§‚å¯Ÿå˜åŒ–
                    )
                    
                    choice = self.parse_judge_result(result)
                    
                    # å°†é€‰æ‹©è½¬æ¢ä¸ºå®é™…é€‰æ‹©çš„æ¨¡å¼
                    if order == "hybrid_first":
                        actual_choice = "hybrid" if choice == "A" else "naive" if choice == "B" else "unclear"
                    else:
                        actual_choice = "naive" if choice == "A" else "hybrid" if choice == "B" else "unclear"
                    
                    pair_results["trials"].append({
                        "trial_num": trial_num + 1,
                        "order": order,
                        "raw_choice": choice,
                        "actual_choice": actual_choice
                    })
                    
                    print(f"    é€‰æ‹©: {actual_choice}")
                    await asyncio.sleep(2)  # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                    
                except Exception as e:
                    print(f"    âŒ è¯•éªŒå¤±è´¥: {e}")
                    await asyncio.sleep(3)
            
            # åˆ†æè¯¥ç­”æ¡ˆå¯¹çš„è¯•æ¬¡ä¸€è‡´æ€§
            actual_choices = [t["actual_choice"] for t in pair_results["trials"] if t["actual_choice"] != "unclear"]
            
            if len(actual_choices) >= 2:
                # è®¡ç®—ä¸€è‡´æ€§
                hybrid_count = actual_choices.count("hybrid")
                naive_count = actual_choices.count("naive")
                
                consistency = max(hybrid_count, naive_count) / len(actual_choices)
                dominant_choice = "hybrid" if hybrid_count > naive_count else "naive" if naive_count > hybrid_count else "tie"
                
                pair_results.update({
                    "total_valid_trials": len(actual_choices),
                    "hybrid_count": hybrid_count,
                    "naive_count": naive_count,
                    "consistency_rate": consistency,
                    "dominant_choice": dominant_choice,
                    "has_trial_bias": consistency < 0.8  # ä¸€è‡´æ€§ä½äº80%è®¤ä¸ºæœ‰è¯•æ¬¡åç½®
                })
                
                if consistency < 0.8:
                    print(f"  ğŸš¨ æ£€æµ‹åˆ°è¯•æ¬¡åç½®! ä¸€è‡´æ€§: {consistency:.2%}")
                else:
                    print(f"  âœ… è¯•æ¬¡ä¸€è‡´æ€§è‰¯å¥½: {consistency:.2%}")
            
            trial_bias_results.append(pair_results)
        
        self.results["trial_bias"] = trial_bias_results
        return trial_bias_results
    
    async def test_hybrid_vs_naive_preference(self, answer_pairs):
        """æµ‹è¯•å¯¹Hybrid vs Naiveçš„æ•´ä½“åå¥½"""
        
        judge_prompt_template = """
è¯·è¯„åˆ¤ä»¥ä¸‹ä¸¤ä¸ªRAGç³»ç»Ÿå›ç­”å“ªä¸ªæ›´å¥½ï¼š

é—®é¢˜ï¼š{question}

Hybridæ¨¡å¼å›ç­”ï¼š{hybrid_answer}

Naiveæ¨¡å¼å›ç­”ï¼š{naive_answer}

è¯·é€‰æ‹©æ›´å¥½çš„å›ç­”ï¼ˆHybridæˆ–Naiveï¼‰å¹¶è¯´æ˜ç†ç”±ï¼š
"""
        
        preference_results = []
        
        print(f"æµ‹è¯•æ‰€æœ‰ {len(answer_pairs)} ä¸ªç­”æ¡ˆå¯¹çš„æ¨¡å¼åå¥½...")
        
        for pair_idx, pair in enumerate(answer_pairs[:10]):  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤šè¯·æ±‚
            print(f"\næµ‹è¯•æ¨¡å¼åå¥½ {pair_idx + 1}/10: {pair['question'][:100]}...")
            
            prompt = judge_prompt_template.format(
                question=pair["question"],
                hybrid_answer=pair["hybrid_answer"],
                naive_answer=pair["naive_answer"]
            )
            
            try:
                result = await lightrag_llm_func_async(
                    prompt,
                    system="ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„è¯„åˆ¤è€…ï¼Œè¯·å®¢è§‚è¯„ä»·RAGç³»ç»Ÿçš„å›ç­”è´¨é‡ã€‚",
                    max_tokens=300,
                    temperature=0.1
                )
                
                # è§£æé€‰æ‹©äº†å“ªä¸ªæ¨¡å¼
                if "hybrid" in result.lower() or "æ··åˆ" in result.lower():
                    choice = "hybrid"
                elif "naive" in result.lower() or "æœ´ç´ " in result.lower():
                    choice = "naive"
                else:
                    choice = "unclear"
                
                preference_results.append({
                    "pair_id": pair_idx,
                    "question": pair["question"][:100],
                    "choice": choice,
                    "hybrid_length": len(pair["hybrid_answer"]),
                    "naive_length": len(pair["naive_answer"]),
                    "length_diff": pair["length_diff"]
                })
                
                print(f"é€‰æ‹©: {choice}")
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"âŒ æ¨¡å¼åå¥½æµ‹è¯•å¤±è´¥: {e}")
                await asyncio.sleep(2)
        
        self.results["hybrid_vs_naive_comparison"] = preference_results
        return preference_results
    
    def parse_judge_result(self, result):
        """è§£æLLMè¯„åˆ¤ç»“æœ"""
        result_lower = result.lower()
        
        if "ç­”æ¡ˆa" in result_lower or "é€‰æ‹©a" in result_lower or "aæ›´å¥½" in result_lower or "aæ¯”è¾ƒå¥½" in result_lower:
            return "A"
        elif "ç­”æ¡ˆb" in result_lower or "é€‰æ‹©b" in result_lower or "bæ›´å¥½" in result_lower or "bæ¯”è¾ƒå¥½" in result_lower:
            return "B"
        else:
            # å°è¯•å…¶ä»–è§£ææ–¹å¼
            a_count = result.count("A") + result.count("a")
            b_count = result.count("B") + result.count("b")
            
            if a_count > b_count:
                return "A"
            elif b_count > a_count:
                return "B"
            else:
                return "UNCLEAR"
    
    async def run_comprehensive_bias_analysis(self, results_dir, questions_file):
        """è¿è¡Œå…¨é¢çš„åç½®åˆ†æï¼ˆä½ç½®ã€é•¿åº¦ã€è¯•æ¬¡ï¼‰"""
        print("ğŸ§ª å¼€å§‹å…¨é¢åç½®åˆ†æï¼ˆä½ç½® + é•¿åº¦ + è¯•æ¬¡ï¼‰")
        
        # åŠ è½½çœŸå®æ•°æ®
        if not self.load_real_data(results_dir, questions_file):
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return None
        
        # æ‰¾åˆ°æ‰€æœ‰Hybrid vs Naiveç­”æ¡ˆå¯¹
        answer_pairs = self.find_all_hybrid_naive_pairs()
        if not answer_pairs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç­”æ¡ˆå¯¹")
            return None
        
        print("\n1ï¸âƒ£ æµ‹è¯•ä½ç½®åç½®...")
        await self.test_position_bias_hybrid_naive(answer_pairs)
        
        print("\n2ï¸âƒ£ æµ‹è¯•é•¿åº¦åç½®...")
        await self.test_length_bias_hybrid_naive(answer_pairs)
        
        print("\n3ï¸âƒ£ æµ‹è¯•è¯•æ¬¡åç½®...")
        await self.test_trial_bias_hybrid_naive(answer_pairs)
        
        print("\n4ï¸âƒ£ æµ‹è¯•æ¨¡å¼åå¥½...")
        await self.test_hybrid_vs_naive_preference(answer_pairs)
        
        return self.results
    
    def generate_comprehensive_bias_report(self):
        """ç”Ÿæˆå…¨é¢çš„åç½®åˆ†ææŠ¥å‘Š"""
        print("\n" + "="*70)
        print("ğŸ” å…¨é¢åç½®åˆ†ææŠ¥å‘Š")
        print("="*70)
        
        # ä½ç½®åç½®æŠ¥å‘Š
        if self.results["position_bias"]:
            position_data = self.results["position_bias"]
            consistent_count = sum(1 for item in position_data if item["consistent"])
            consistency_rate = consistent_count / len(position_data) if position_data else 0
            
            print(f"\nğŸ“ ä½ç½®åç½®åˆ†æ:")
            print(f"  æ€»æµ‹è¯•ç­”æ¡ˆå¯¹: {len(position_data)}")
            print(f"  åˆ¤æ–­ä¸€è‡´æ€§: {consistency_rate:.2%}")
            print(f"  åç½®ç¨‹åº¦: {(1-consistency_rate)*100:.1f}%")
            
            if consistency_rate < 0.7:
                print("  âš ï¸  æ£€æµ‹åˆ°ä¸¥é‡ä½ç½®åç½®ï¼")
            else:
                print("  âœ…  ä½ç½®åç½®åœ¨å¯æ¥å—èŒƒå›´å†…")
        
        # é•¿åº¦åç½®æŠ¥å‘Š
        if self.results["length_bias"]:
            length_data = self.results["length_bias"]
            total_tests = len(length_data) * 2
            chose_longer_count = sum(1 for item in length_data if item["chose_longer_when_short_first"]) + \
                               sum(1 for item in length_data if item["chose_longer_when_long_first"])
            longer_preference = chose_longer_count / total_tests if total_tests > 0 else 0
            
            print(f"\nğŸ“ é•¿åº¦åç½®åˆ†æ:")
            print(f"  æµ‹è¯•ç­”æ¡ˆå¯¹æ•°: {len(length_data)}")
            print(f"  åå¥½é•¿ç­”æ¡ˆæ¯”ä¾‹: {longer_preference:.2%}")
            print(f"  é•¿åº¦åç½®ç¨‹åº¦: {abs(longer_preference - 0.5)*200:.1f}%")
            
            if longer_preference > 0.7 or longer_preference < 0.3:
                print("  âš ï¸  æ£€æµ‹åˆ°æ˜æ˜¾çš„é•¿åº¦åç½®ï¼")
            else:
                print("  âœ…  é•¿åº¦åç½®åœ¨å¯æ¥å—èŒƒå›´å†…")
        
        # è¯•æ¬¡åç½®æŠ¥å‘Š
        if self.results["trial_bias"]:
            trial_data = self.results["trial_bias"]
            total_pairs = len(trial_data)
            biased_pairs = sum(1 for item in trial_data if item.get("has_trial_bias", False))
            
            # è®¡ç®—å¹³å‡ä¸€è‡´æ€§
            consistency_rates = [item.get("consistency_rate", 0) for item in trial_data if "consistency_rate" in item]
            avg_consistency = sum(consistency_rates) / len(consistency_rates) if consistency_rates else 0
            
            print(f"\nğŸ”„ è¯•æ¬¡åç½®åˆ†æ:")
            print(f"  æµ‹è¯•ç­”æ¡ˆå¯¹æ•°: {total_pairs}")
            print(f"  å¹³å‡è¯•æ¬¡ä¸€è‡´æ€§: {avg_consistency:.2%}")
            print(f"  å­˜åœ¨è¯•æ¬¡åç½®çš„ç­”æ¡ˆå¯¹: {biased_pairs} ({biased_pairs/total_pairs*100:.1f}%)")
            
            if avg_consistency < 0.7:
                print("  âš ï¸  æ£€æµ‹åˆ°ä¸¥é‡çš„è¯•æ¬¡åç½®ï¼")
            elif biased_pairs / total_pairs > 0.3:
                print("  âš ï¸  æ£€æµ‹åˆ°ä¸­ç­‰ç¨‹åº¦çš„è¯•æ¬¡åç½®")
            else:
                print("  âœ…  è¯•æ¬¡åç½®åœ¨å¯æ¥å—èŒƒå›´å†…")
        
        # æ¨¡å¼åå¥½æŠ¥å‘Š
        if self.results["hybrid_vs_naive_comparison"]:
            preference_data = self.results["hybrid_vs_naive_comparison"]
            hybrid_count = sum(1 for item in preference_data if item["choice"] == "hybrid")
            naive_count = sum(1 for item in preference_data if item["choice"] == "naive")
            
            print(f"\nğŸ”„ æ¨¡å¼åå¥½åˆ†æ:")
            print(f"  åå¥½Hybrid: {hybrid_count} ({hybrid_count/len(preference_data)*100:.1f}%)")
            print(f"  åå¥½Naive: {naive_count} ({naive_count/len(preference_data)*100:.1f}%)")
        
        # ç»¼åˆåç½®è¯„ä¼°
        print(f"\nğŸ“Š ç»¼åˆåç½®è¯„ä¼°:")
        bias_score = 0
        
        # ä½ç½®åç½®è¯„åˆ†
        position_data = self.results.get("position_bias", [])
        if position_data:
            consistent_count = sum(1 for item in position_data if item["consistent"])
            consistency_rate = consistent_count / len(position_data)
            
            if consistency_rate < 0.7:
                bias_score += 30
                print("  - ä½ç½®åç½®: ä¸¥é‡ (+30åˆ†)")
            elif consistency_rate < 0.8:
                bias_score += 15
                print("  - ä½ç½®åç½®: ä¸­ç­‰ (+15åˆ†)")
            else:
                print("  - ä½ç½®åç½®: è½»å¾® (+0åˆ†)")
        
        # é•¿åº¦åç½®è¯„åˆ†
        length_data = self.results.get("length_bias", [])
        if length_data:
            total_tests = len(length_data) * 2
            chose_longer_count = sum(1 for item in length_data if item["chose_longer_when_short_first"]) + \
                               sum(1 for item in length_data if item["chose_longer_when_long_first"])
            longer_preference = chose_longer_count / total_tests if total_tests > 0 else 0.5
            
            if longer_preference > 0.7 or longer_preference < 0.3:
                bias_score += 25
                print("  - é•¿åº¦åç½®: ä¸¥é‡ (+25åˆ†)")
            elif longer_preference > 0.6 or longer_preference < 0.4:
                bias_score += 10
                print("  - é•¿åº¦åç½®: ä¸­ç­‰ (+10åˆ†)")
            else:
                print("  - é•¿åº¦åç½®: è½»å¾® (+0åˆ†)")
        
        # è¯•æ¬¡åç½®è¯„åˆ†
        trial_data = self.results.get("trial_bias", [])
        if trial_data:
            consistency_rates = [item.get("consistency_rate", 0) for item in trial_data if "consistency_rate" in item]
            avg_consistency = sum(consistency_rates) / len(consistency_rates) if consistency_rates else 1.0
            biased_pairs = sum(1 for item in trial_data if item.get("has_trial_bias", False))
            total_pairs = len(trial_data)
            
            if avg_consistency < 0.7:
                bias_score += 20
                print("  - è¯•æ¬¡åç½®: ä¸¥é‡ (+20åˆ†)")
            elif biased_pairs / total_pairs > 0.3:
                bias_score += 10
                print("  - è¯•æ¬¡åç½®: ä¸­ç­‰ (+10åˆ†)")
            else:
                print("  - è¯•æ¬¡åç½®: è½»å¾® (+0åˆ†)")
        
        print(f"\nğŸ¯ æ€»åç½®åˆ†æ•°: {bias_score}/75")
        if bias_score >= 50:
            print("  âŒ è¯„åˆ¤ç³»ç»Ÿå­˜åœ¨ä¸¥é‡åç½®ï¼Œéœ€è¦é‡æ–°è®¾è®¡")
        elif bias_score >= 25:
            print("  âš ï¸  è¯„åˆ¤ç³»ç»Ÿå­˜åœ¨ä¸­ç­‰åç½®ï¼Œå»ºè®®æ”¹è¿›")
        else:
            print("  âœ… è¯„åˆ¤ç³»ç»Ÿåç½®å¯æ¥å—")
        
        # æ”¹è¿›å»ºè®®
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        if any(consistency_rate < 0.8 for consistency_rate in [consistency_rate] if position_data):
            print("  - å®æ–½åŒå‘è¯„åˆ¤ç­–ç•¥å‡å°‘ä½ç½®åç½®")
        if any(longer_preference > 0.6 or longer_preference < 0.4 for longer_preference in [longer_preference] if length_data):
            print("  - åœ¨è¯„åˆ¤æŒ‡ä»¤ä¸­å¼ºè°ƒè´¨é‡è€Œéé•¿åº¦")
        if any(avg_consistency < 0.8 for avg_consistency in [avg_consistency] if trial_data):
            print("  - é™ä½temperatureå‚æ•°æé«˜è¯„åˆ¤ä¸€è‡´æ€§")
            print("  - ä½¿ç”¨å¤šæ¬¡è¯„åˆ¤å¹¶å–ä¼—æ•°ç»“æœ")
        print("  - ä½¿ç”¨ç›²è¯„æ–¹å¼éšè—æ¨¡å¼ä¿¡æ¯")
        print("  - è€ƒè™‘ä½¿ç”¨ä¸“é—¨è®­ç»ƒçš„è¯„åˆ¤æ¨¡å‹")
    
    def save_results(self, output_file):
        """ä¿å­˜åˆ†æç»“æœ"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        self.generate_comprehensive_bias_report()

async def main():
    """ä¸»å‡½æ•°"""
    analyzer = JudgeBiasAnalyzer()
    
    # è®¾ç½®æ•°æ®è·¯å¾„
    results_dir = "../exp_results/data/results"
    questions_file = "../exp_results/data/questions/cs_questions.txt"
    
    # è¿è¡Œå…¨é¢åç½®åˆ†æï¼ˆåŒ…å«ä¸‰ç§åç½®æµ‹è¯•ï¼‰
    results = await analyzer.run_comprehensive_bias_analysis(results_dir, questions_file)
    
    if results:
        # ä¿å­˜ç»“æœ
        output_file = "../exp_results/data/evaluations/comprehensive_bias_analysis.json"
        analyzer.save_results(output_file)
        
        print(f"\nğŸ’¾ å…¨é¢åç½®åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    else:
        print("âŒ åç½®åˆ†æå¤±è´¥")

if __name__ == "__main__":
    asyncio.run(main())